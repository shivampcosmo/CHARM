{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "# root_dir = '/mnt/home/spandey/ceph/CHARM/'\n",
    "# os.chdir(root_dir)\n",
    "import pickle as pk\n",
    "import pathlib\n",
    "curr_path = pathlib.Path().absolute()\n",
    "src_path = os.path.abspath(curr_path / \"../../charm/\") \n",
    "sys.path.append(src_path)\n",
    "# sys.path.append(root_dir)\n",
    "# from combined_models import *\n",
    "# from all_models import *\n",
    "from charm import *\n",
    "from utils_data_prep_cosmo_vel import *\n",
    "from colossus.cosmology import cosmology\n",
    "params = {'flat': True, 'H0': 67.11, 'Om0': 0.3175, 'Ob0': 0.049, 'sigma8': 0.834, 'ns': 0.9624}\n",
    "cosmo = cosmology.setCosmology('myCosmo', **params)\n",
    "from colossus.lss import mass_function\n",
    "from tqdm import tqdm\n",
    "import sparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 3\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_config_name = 'TRAIN_MASS_FREECOSMO_cond_fastpm_ns128.yaml'\n",
    "run_config_name = 'TRAIN_MASS_FREECOSMO_cond_fastpm_ns128_lresdata.yaml'\n",
    "with open(\"/mnt/home/spandey/ceph/CHARM/run_configs/\" + run_config_name,\"r\") as file_object:\n",
    "    config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "config_sims = config['sim_settings']\n",
    "ji_array = np.arange(int(config_sims['nsims']))\n",
    "nsubvol_per_ji = int(config_sims['nsubvol_per_ji'])\n",
    "nsubvol_fid = int(config_sims['nsubvol_fid'])\n",
    "subsel_criteria = config_sims['subsel_criteria']\n",
    "num_cosmo_params = int(config_sims['num_cosmo_params'])\n",
    "ns_d = config_sims['ns_d']\n",
    "nb = config_sims['nb']\n",
    "nax_d =  ns_d // nb\n",
    "nf = config_sims['nf']\n",
    "layers_types = config_sims['layers_types']\n",
    "z_inference = config_sims['z_inference']\n",
    "nc = 0\n",
    "for jl in range(len(layers_types)):\n",
    "    if layers_types[jl] == 'cnn':\n",
    "        nc += 1\n",
    "    elif layers_types[jl] == 'res':\n",
    "        nc += 2\n",
    "    else:\n",
    "        raise ValueError(\"layer type not supported\")\n",
    "\n",
    "z_all = config_sims['z_all']\n",
    "z_all_FP = config_sims['z_all_FP']\n",
    "# z_all_FP = z_all_FP[:-1]\n",
    "z_all_FP = z_all_FP\n",
    "ns_h = config_sims['ns_h']\n",
    "nax_h = ns_h // nb\n",
    "cond_sim = config_sims['cond_sim']\n",
    "\n",
    "nsims_per_batch = config_sims['nsims_per_batch']\n",
    "nbatches_train = config_sims['nbatches_train']\n",
    "\n",
    "mass_type = config_sims['mass_type']\n",
    "lgMmin = config_sims['lgMmin']\n",
    "lgMmax = config_sims['lgMmax']\n",
    "stype = config_sims['stype']\n",
    "rescale_sub = config_sims['rescale_sub']\n",
    "lgMmincutstr = config_sims['lgMmincutstr']\n",
    "# subsel_highM1 = config_sims['subsel_highM1']\n",
    "# nsubsel = config_sims['nsubsel']\n",
    "is_HR = config_sims['is_HR']\n",
    "\n",
    "try:\n",
    "    Nmax = config_sims['Nmax']\n",
    "except:\n",
    "    Nmax = 4\n",
    "\n",
    "config_net = config['network_settings']\n",
    "hidden_dim_MAF = config_net['hidden_dim_MAF']\n",
    "learning_rate = config_net['learning_rate']\n",
    "K_M1 = config_net['K_M1']\n",
    "B_M1 = config_net['B_M1']\n",
    "nflows_M1_NSF = config_net['nflows_M1_NSF']\n",
    "\n",
    "K_Mdiff = config_net['K_Mdiff']\n",
    "B_Mdiff = config_net['B_Mdiff']\n",
    "nflows_Mdiff_NSF = config_net['nflows_Mdiff_NSF']\n",
    "\n",
    "base_dist_Ntot = config_net['base_dist_Ntot']\n",
    "if base_dist_Ntot == 'None':\n",
    "    base_dist_Ntot = None\n",
    "base_dist_M1 = config_net['base_dist_M1']\n",
    "base_dist_Mdiff = config_net['base_dist_Mdiff']\n",
    "ngauss_M1 = config_net['ngauss_M1']\n",
    "\n",
    "changelr = config_net['changelr']\n",
    "ksize = nf\n",
    "nfeature_cnn = config_net['nfeature_cnn']\n",
    "nout_cnn = 4 * nfeature_cnn\n",
    "if cond_sim == 'fastpm':\n",
    "    if any('v' in str(string) for string in z_all_FP):\n",
    "        ninp = len(z_all_FP) + 2\n",
    "    else:\n",
    "        ninp = len(z_all_FP)\n",
    "\n",
    "elif cond_sim == 'quijote':\n",
    "    ninp = len(z_all)\n",
    "else:\n",
    "    raise ValueError(\"cond_sim not supported\")\n",
    "\n",
    "num_cond = nout_cnn + ninp + num_cosmo_params\n",
    "\n",
    "\n",
    "device_id = torch.device(\"cuda\")\n",
    "ndim_diff = Nmax - 1\n",
    "\n",
    "lgM_array = np.linspace(lgMmin, lgMmax, 1000)\n",
    "M_array = 10**lgM_array\n",
    "if '200c' in mass_type:\n",
    "    hmf = mass_function.massFunction(M_array, float(z_inference), mdef = '200c', model = 'tinker08', q_out = 'dndlnM')\n",
    "if 'vir' in mass_type:\n",
    "    hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'vir', model = 'tinker08', q_out = 'dndlnM')    \n",
    "if 'fof' in mass_type:\n",
    "    hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'fof', model = 'bhattacharya11', q_out = 'dndlnM')\n",
    "lgM_rescaled = rescale_sub + (lgM_array - lgMmin)/(lgMmax-lgMmin)\n",
    "\n",
    "int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
    "hmf_pdf = hmf/int_val\n",
    "# define the cdf of the halo mass function\n",
    "hmf_cdf = np.zeros_like(hmf_pdf)\n",
    "for i in range(len(hmf_cdf)):\n",
    "    hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n",
    "\n",
    "if 'sigv' in config:\n",
    "    sigv = config['sigv']\n",
    "else:\n",
    "    sigv = 0.05\n",
    "num_cond_Ntot = num_cond\n",
    "mu_all = np.arange(Nmax + 1) + 1\n",
    "sig_all = sigv * np.ones_like(mu_all)\n",
    "ngauss_Nhalo = Nmax + 1\n",
    "\n",
    "model_BinaryMask = SumGaussModel(\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_Ntot,\n",
    "    ngauss=2,\n",
    "    mu_all=mu_all[:2],\n",
    "    sig_all=sig_all[:2],\n",
    "    base_dist=base_dist_Ntot,\n",
    "    device=device_id\n",
    "    )\n",
    "\n",
    "# model_BinaryMask.to(dev)\n",
    "\n",
    "\n",
    "model_multiclass = SumGaussModel(\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_Ntot,\n",
    "    ngauss=ngauss_Nhalo - 1,\n",
    "    mu_all=mu_all[1:] - 1,\n",
    "    sig_all=sig_all[1:],\n",
    "    base_dist=base_dist_Ntot,\n",
    "    device=device_id\n",
    "    )\n",
    "\n",
    "\n",
    "# model_multiclass.to(dev)\n",
    "\n",
    "\n",
    "num_cond_M1 = num_cond + 1\n",
    "# # if conditioned on fastpm we will also give the fastpm fof M1 halos and its mask as conditional\n",
    "# if cond_sim == 'fastpm':\n",
    "#     num_cond_M1 += 2\n",
    "\n",
    "model_M1 = NSF_1var_CNNcond(\n",
    "    K=K_M1,\n",
    "    B=B_M1,\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_M1,\n",
    "    nflows=nflows_M1_NSF,\n",
    "    base_dist=base_dist_M1,\n",
    "    ngauss=ngauss_M1,\n",
    "    lgM_rs_tointerp=lgM_rescaled,\n",
    "    hmf_pdf_tointerp=hmf_pdf,\n",
    "    hmf_cdf_tointerp=hmf_cdf,\n",
    "    device=device_id \n",
    "    )\n",
    "\n",
    "ndim_diff = Nmax - 1\n",
    "num_cond_Mdiff = num_cond + 2\n",
    "model_Mdiff = NSF_Autoreg_CNNcond(\n",
    "    dim=ndim_diff,\n",
    "    K=K_Mdiff,\n",
    "    B=B_Mdiff,\n",
    "    hidden_dim=hidden_dim_MAF,\n",
    "    num_cond=num_cond_Mdiff,\n",
    "    nflows=nflows_Mdiff_NSF,\n",
    "    base_dist=base_dist_Mdiff,\n",
    "    mu_pos=True\n",
    "    )\n",
    "\n",
    "\n",
    "ndim = ndim_diff + 1\n",
    "model = COMBINED_Model(\n",
    "    None,\n",
    "    model_Mdiff,\n",
    "    # None,\n",
    "    model_M1,\n",
    "    model_BinaryMask,\n",
    "    model_multiclass,\n",
    "    ndim,\n",
    "    ksize,\n",
    "    ns_d,\n",
    "    ns_h,\n",
    "    1,\n",
    "    ninp,\n",
    "    nfeature_cnn,\n",
    "    nout_cnn,\n",
    "    layers_types=layers_types,\n",
    "    act='tanh',\n",
    "    padding='valid',\n",
    "    sep_Binary_cond=True,\n",
    "    sep_MultiClass_cond=True,\n",
    "    sep_M1_cond=True,\n",
    "    sep_Mdiff_cond=True,\n",
    "    num_cond_Binary = num_cond_Ntot,\n",
    "    num_cond_MultiClass = num_cond_Ntot,\n",
    "    num_cond_M1 = num_cond_M1,\n",
    "    num_cond_Mdiff = num_cond_Mdiff\n",
    "    ).to(device_id)\n",
    "\n",
    "# model = DDP(model, device_ids=[device_id], find_unused_parameters=True)\n",
    "\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test0/'\n",
    "\n",
    "checkpoint = torch.load(ldir_cp + f'test_model_bestfit_6484.pth', map_location=device_id)\n",
    "# print(iter)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ldir_f = '/mnt/home/spandey/ceph/Quijote/data_NGP_self_fastpm_LH/'\n",
    "\n",
    "n_dim_red = (nf - 1) // 2\n",
    "n_pad = n_dim_red * nc\n",
    "\n",
    "df_zg = pk.load(open(ldir_f + '%d/density_HR_full_m_res_128_z=0.5_nbatch_8_nfilter_3_ncnn_0.pk'%test_id,'rb'))\n",
    "df_test_zg = df_zg['density_cic_unpad_combined']\n",
    "df_test_pad_zg = np.pad(df_test_zg, n_pad, 'wrap')\n",
    "\n",
    "\n",
    "\n",
    "z_REDSHIFT = float(z_all_FP[-1].split('_')[1])\n",
    "if z_REDSHIFT == 0.0:\n",
    "    z_REDSHIFT = 0\n",
    "\n",
    "df_load = pk.load(open(\n",
    "    ldir_f + '/' + str(test_id) + '/velocity_HR_full_m_res_128_z=' + str(z_REDSHIFT) + '_nbatch_8_nfilter_3_ncnn_0.pk', 'rb')\n",
    "    )\n",
    "\n",
    "vel_all = df_load['velocity_cic_unpad_combined']\n",
    "# # vel_pad_all = \n",
    "vel_pad = np.stack([np.pad(vel_all[j,...], n_pad, 'wrap') for j in range(3)], axis=0)\n",
    "\n",
    "df_test_all_pad = np.concatenate([np.log(1 + df_test_pad_zg + 1e-10)[None,...], vel_pad], axis=0)[None, None,:]\n",
    "# df_test_all_pad = np.stack([np.log(1 + df_test_pad_zg + 1e-10), np.log(\n",
    "#     1 + df_test_pad_zIC + 1e-10), df_test_pad_constrast_zg], axis=0)[None, None, :]\n",
    "\n",
    "# df_test_all_pad.shape\n",
    "\n",
    "# density_smoothed = gaussian_filter(df_test_zg, sigma=VALUE_SIG)\n",
    "# df_test_constrast_zg = density_smoothed - df_test_zg\n",
    "# \n",
    "df_test_all_unpad = np.concatenate([np.log(1 + df_test_zg + 1e-10)[None,...], vel_all], axis=0)[None, None,:]\n",
    "# df_test_all_unpad = np.stack([np.log(1 + df_test_zg + 1e-10), np.log(\n",
    "#     1 + df_test_zIC + 1e-10), df_test_constrast_zg], axis=0)[None, None, :]\n",
    "\n",
    "# df_test_all_unpad.shape\n",
    "\n",
    "cond_nsh_test = np.moveaxis(df_test_all_unpad, 2, 5)\n",
    "nsims_test = cond_nsh_test.shape[1]\n",
    "nax_h_test = cond_nsh_test.shape[2]\n",
    "ninp_test = cond_nsh_test.shape[-1]\n",
    "cond_tensor_nsh_test = torch.Tensor(np.copy(cond_nsh_test.reshape(1,nsims_test * (nax_h_test ** 3), ninp_test))).cuda(device_id)    \n",
    "\n",
    "# cond_tensor_nsh_test.shape\n",
    "LH_cosmo_val_file='/mnt/home/spandey/ceph/Quijote/latin_hypercube_params.txt'\n",
    "LH_cosmo_val_all = np.loadtxt(LH_cosmo_val_file)\n",
    "fid_cosmo_val_all = LH_cosmo_val_all[test_id]\n",
    "# fid_cosmo_val_all = np.array([0.3175, 0.049, 0.6711, 0.9624, 0.64])  \n",
    "\n",
    "cosmo_val_test = np.tile(fid_cosmo_val_all, (cond_tensor_nsh_test.shape[1] ,1))[None,:]\n",
    "\n",
    "# cosmo_val_test.shape\n",
    "# df_test_all_pad.shape, df_test_all_unpad.shape, cosmo_val_test.shape\n",
    "df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
    "df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n",
    "cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n",
    "\n",
    "\n",
    "train_Ntot, train_M1, train_Mdiff = 1, 1, 1\n",
    "train_binary, train_multi = 1, 1\n",
    "# if verbose:\n",
    "    # print(f\"Running the model\")\n",
    "\n",
    "# run the model\n",
    "Ntot_samp_test, M1_samp_test, M_diff_samp_test, mask_tensor_M1_samp_test, mask_tensor_Mdiff_samp_test, _ = model.module.inverse(\n",
    "    cond_x=df_test_all_pad,\n",
    "    cond_x_nsh=df_test_all_unpad,\n",
    "    cond_cosmo=cosmo_val_test,\n",
    "    use_truth_Nhalo=1-train_Ntot,\n",
    "    use_truth_M1=1-train_M1,\n",
    "    use_truth_Mdiff=1-train_Mdiff,\n",
    "    mask_Mdiff_truth=None,\n",
    "    mask_M1_truth=None,\n",
    "    Nhalos_truth=None,\n",
    "    M1_truth=None,\n",
    "    Mdiff_truth=None,\n",
    "    train_binary=train_binary,\n",
    "    train_multi=train_multi,\n",
    "    train_M1=train_M1,\n",
    "    train_Mdiff=train_Mdiff,\n",
    ")\n",
    "\n",
    "\n",
    "BoxSize=1000\n",
    "# Ntot_samp_test[0].shape\n",
    "Ntot_samp_test_rs = Ntot_samp_test[0][:, np.newaxis]\n",
    "M1_samp_test_rs = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:, 0]).cpu().detach().numpy()\n",
    "M_diff_samp_test_rs = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ntot_samp_test = Ntot_samp_test[0][:, np.newaxis]\n",
    "save_subvol_Nhalo = Ntot_samp_test.reshape(\n",
    "    nsims_test, nax_h_test, nax_h_test, nax_h_test)\n",
    "save_subvol_M1 = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:, 0]\n",
    "                    ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, 1)\n",
    "save_subvol_Mdiff = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]\n",
    "                        ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff)\n",
    "\n",
    "mask_subvol_Mtot1 = mask_tensor_M1_samp_test[0].cpu().detach().numpy().reshape(\n",
    "    nsims_test, nax_h_test, nax_h_test, nax_h_test)[..., None]\n",
    "mask_subvol_Mtot2 = mask_tensor_Mdiff_samp_test[0].cpu().detach().numpy(\n",
    ").reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff)\n",
    "mask_subvol_Mtot = np.concatenate(\n",
    "    [mask_subvol_Mtot1, mask_subvol_Mtot2], axis=-1)\n",
    "\n",
    "# compute the mass of halos from output\n",
    "save_subvol_Mtot = np.zeros(\n",
    "    (nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff + 1))\n",
    "# Mmin, Mmax = return_dict_test['Mmin'], return_dict_test['Mmax']\n",
    "for jd in range(ndim_diff + 1):\n",
    "    if jd == 0:\n",
    "        save_subvol_Mtot[..., jd] = (\n",
    "            save_subvol_M1[..., 0] + 0.5) * (lgMmax - lgMmin) + lgMmin\n",
    "    else:\n",
    "        save_subvol_Mtot[...,\n",
    "                            jd] = (save_subvol_Mtot[..., jd - 1]) - (save_subvol_Mdiff[..., jd - 1]) * (lgMmax - lgMmin)\n",
    "\n",
    "save_subvol_Mtot *= mask_subvol_Mtot\n",
    "\n",
    "Nhalos = save_subvol_Nhalo[0, ...]  # histogram of halos in each voxel\n",
    "M_halos = save_subvol_Mtot[0, ...]  # mass of halos in each voxel\n",
    "M_halos_sort_norm = rescale_sub + (M_halos - lgMmin)/(lgMmax - lgMmin)\n",
    "M_halos_sort_norm *= mask_subvol_Mtot[0, ...]\n",
    "M_halos_sort_norm_condvel = M_halos_sort_norm.reshape(nax_h_test**3, -1)\n",
    "# create the meshgrid\n",
    "xall = (np.linspace(0, BoxSize, ns_h + 1))\n",
    "xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "yarray = np.copy(xarray)\n",
    "zarray = np.copy(xarray)\n",
    "x_cy, y_cy, z_cy = np.meshgrid(xarray, yarray, zarray, indexing='ij')\n",
    "\n",
    "# record discrete halo positions and masses\n",
    "x_h_mock, y_h_mock, z_h_mock, lgM_mock = [], [], [], []\n",
    "# Nmax_sel = 3\n",
    "k = 0\n",
    "for jx in range(ns_h):\n",
    "    for jy in range(ns_h):\n",
    "        for jz in range(ns_h):\n",
    "            Nh_vox = int(Nhalos[jx, jy, jz])\n",
    "            if Nh_vox > 0:\n",
    "                x_h_mock.append(x_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                y_h_mock.append(y_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                z_h_mock.append(z_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "\n",
    "                lgM_mock.append((M_halos[jx, jy, jz, :Nh_vox]))\n",
    "                k += Nh_vox\n",
    "\n",
    "# convert to numpy arrays\n",
    "x_h_mock = np.concatenate(x_h_mock)\n",
    "y_h_mock = np.concatenate(y_h_mock)\n",
    "z_h_mock = np.concatenate(z_h_mock)\n",
    "pos_h_mock = np.vstack((x_h_mock, y_h_mock, z_h_mock)).T\n",
    "lgMass_mock = np.concatenate(lgM_mock)\n",
    "# convert to float data type\n",
    "pos_h_mock = pos_h_mock.astype('float32')\n",
    "lgMass_mock = lgMass_mock.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "isim = test_id\n",
    "# snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/fiducial_HR'\n",
    "snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/latin_hypercube_HR'\n",
    "snapnum = 3\n",
    "MAS     = 'NGP'  #mass-assigment scheme\n",
    "verbose = False   #print information on progress\n",
    "snapdir = snap_dir_base + '/' + str(isim)  #folder hosting the catalogue\n",
    "rockstar = np.loadtxt(snapdir + '/out_' + str(snapnum) + '_pid.list')\n",
    "with open(snapdir + '/out_' + str(snapnum) + '_pid.list', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "header = lines[0].split()\n",
    "# get the properties of the halos\n",
    "pos_h_truth = rockstar[:,header.index('X'):header.index('Z')+1]\n",
    "index_M = header.index('M200c')                    \n",
    "mass_truth = rockstar[:,index_M]  #Halo masses in Msun/h\n",
    "lgMass_truth = np.log10(mass_truth).astype(np.float32)\n",
    "vel_h_truth = rockstar[:,header.index('VX'):header.index('VZ')+1]\n",
    "\n",
    "indsel = np.where(mass_truth > 5e12)[0]\n",
    "pos_h_truth = pos_h_truth[indsel]\n",
    "vel_h_truth = vel_h_truth[indsel]\n",
    "lgMass_truth = lgMass_truth[indsel]\n",
    "\n",
    "\n",
    "# saved = {'pos_h_mock': pos_h_mock, 'lgMass_mock': lgMass_mock, 'pos_h_truth': pos_h_truth, \n",
    "#         'lgMass_truth': lgMass_truth, 'vel_h_truth': vel_h_truth,\n",
    "#         'Ntot_samp_test_rs':Ntot_samp_test_rs,\n",
    "#         'M_halos_sort_norm_condvel':M_halos_sort_norm_condvel,\n",
    "#         'df_test_all_pad':df_test_all_pad,\n",
    "#         'df_test_all_unpad':df_test_all_unpad,\n",
    "#         'cosmo_val_test':cosmo_val_test}\n",
    "# pk.dump(saved, open(f'mock_true_data_LH_{test_id}.pk', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ili-sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
