{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from time import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import pickle as pk\n",
    "import pathlib\n",
    "curr_path = pathlib.Path().absolute()\n",
    "src_path = os.path.abspath(curr_path / \"../charm/\") \n",
    "sys.path.append(src_path)\n",
    "# from charm import SumGaussModel, COMBINED_Model, NSF_1var_CNNcond, NSF_Autoreg_CNNcond, COMBINED_Model_vel_only\n",
    "from all_models import SumGaussModel, NSF_1var_CNNcond, NSF_Autoreg_CNNcond\n",
    "from combined_models import COMBINED_Model, COMBINED_Model_vel_only\n",
    "# from utils_data_prep_cosmo_vel import *\n",
    "from colossus.cosmology import cosmology\n",
    "params = {'flat': True, 'H0': 67.11, 'Om0': 0.3175, 'Ob0': 0.049, 'sigma8': 0.834, 'ns': 0.9624}\n",
    "cosmo = cosmology.setCosmology('myCosmo', **params)\n",
    "from colossus.lss import mass_function\n",
    "from tqdm import tqdm\n",
    "import sparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import matplotlib.pyplot as pl\n",
    "import scipy as sp\n",
    "import os\n",
    "\n",
    "def save_cats(test_id, verbose=False):\n",
    "\n",
    "    import pickle as pk\n",
    "    # save_fname_cats = f'/mnt/home/spandey/ceph/CHARM/data/halo_cats_charm_truth/halo_cat_pos_vel_LH_{test_id}.pk'\n",
    "    save_fname_cats = f'/mnt/home/spandey/ceph/CHARM/data/halo_cats_charm_truth_nsubv_vel_10k/halo_cat_pos_vel_LH_{test_id}.pk'    \n",
    "    # if file does not exist:\n",
    "    if not os.path.exists(save_fname_cats):\n",
    "        try:\n",
    "            print('LH: ', test_id)\n",
    "\n",
    "            ti = time()\n",
    "            t0 = time()\n",
    "            run_config_name = 'TRAIN_MASS_FREECOSMO_cond_fastpm_ns128_lresdata.yaml'\n",
    "            with open(\"/mnt/home/spandey/ceph/CHARM/run_configs/\" + run_config_name,\"r\") as file_object:\n",
    "                config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "            config_sims = config['sim_settings']\n",
    "            ji_array = np.arange(int(config_sims['nsims']))\n",
    "            nsubvol_per_ji = int(config_sims['nsubvol_per_ji'])\n",
    "            nsubvol_fid = int(config_sims['nsubvol_fid'])\n",
    "            subsel_criteria = config_sims['subsel_criteria']\n",
    "            num_cosmo_params = int(config_sims['num_cosmo_params'])\n",
    "            ns_d = config_sims['ns_d']\n",
    "            nb = config_sims['nb']\n",
    "            nax_d =  ns_d // nb\n",
    "            nf = config_sims['nf']\n",
    "            layers_types = config_sims['layers_types']\n",
    "            z_inference = config_sims['z_inference']\n",
    "            nc = 0\n",
    "            for jl in range(len(layers_types)):\n",
    "                if layers_types[jl] == 'cnn':\n",
    "                    nc += 1\n",
    "                elif layers_types[jl] == 'res':\n",
    "                    nc += 2\n",
    "                else:\n",
    "                    raise ValueError(\"layer type not supported\")\n",
    "\n",
    "            z_all = config_sims['z_all']\n",
    "            z_all_FP = config_sims['z_all_FP']\n",
    "            # z_all_FP = z_all_FP[:-1]\n",
    "            z_all_FP = z_all_FP\n",
    "            ns_h = config_sims['ns_h']\n",
    "            nax_h = ns_h // nb\n",
    "            cond_sim = config_sims['cond_sim']\n",
    "\n",
    "            nsims_per_batch = config_sims['nsims_per_batch']\n",
    "            nbatches_train = config_sims['nbatches_train']\n",
    "\n",
    "            mass_type = config_sims['mass_type']\n",
    "            lgMmin = config_sims['lgMmin']\n",
    "            lgMmax = config_sims['lgMmax']\n",
    "            stype = config_sims['stype']\n",
    "            rescale_sub = config_sims['rescale_sub']\n",
    "            lgMmincutstr = config_sims['lgMmincutstr']\n",
    "            # subsel_highM1 = config_sims['subsel_highM1']\n",
    "            # nsubsel = config_sims['nsubsel']\n",
    "            is_HR = config_sims['is_HR']\n",
    "\n",
    "            try:\n",
    "                Nmax = config_sims['Nmax']\n",
    "            except:\n",
    "                Nmax = 4\n",
    "\n",
    "            config_net = config['network_settings']\n",
    "            hidden_dim_MAF = config_net['hidden_dim_MAF']\n",
    "            learning_rate = config_net['learning_rate']\n",
    "            K_M1 = config_net['K_M1']\n",
    "            B_M1 = config_net['B_M1']\n",
    "            nflows_M1_NSF = config_net['nflows_M1_NSF']\n",
    "\n",
    "            K_Mdiff = config_net['K_Mdiff']\n",
    "            B_Mdiff = config_net['B_Mdiff']\n",
    "            nflows_Mdiff_NSF = config_net['nflows_Mdiff_NSF']\n",
    "\n",
    "            base_dist_Ntot = config_net['base_dist_Ntot']\n",
    "            if base_dist_Ntot == 'None':\n",
    "                base_dist_Ntot = None\n",
    "            base_dist_M1 = config_net['base_dist_M1']\n",
    "            base_dist_Mdiff = config_net['base_dist_Mdiff']\n",
    "            ngauss_M1 = config_net['ngauss_M1']\n",
    "\n",
    "            changelr = config_net['changelr']\n",
    "            ksize = nf\n",
    "            nfeature_cnn = config_net['nfeature_cnn']\n",
    "            nout_cnn = 4 * nfeature_cnn\n",
    "            if cond_sim == 'fastpm':\n",
    "                if any('v' in str(string) for string in z_all_FP):\n",
    "                    ninp = len(z_all_FP) + 2\n",
    "                else:\n",
    "                    ninp = len(z_all_FP)\n",
    "\n",
    "            elif cond_sim == 'quijote':\n",
    "                ninp = len(z_all)\n",
    "            else:\n",
    "                raise ValueError(\"cond_sim not supported\")\n",
    "\n",
    "            num_cond = nout_cnn + ninp + num_cosmo_params\n",
    "\n",
    "\n",
    "            device_id = torch.device(\"cuda\")\n",
    "            # device_id = torch.device(\"cpu\")    \n",
    "            ndim_diff = Nmax - 1\n",
    "\n",
    "            lgM_array = np.linspace(lgMmin, lgMmax, 100)\n",
    "            M_array = 10**lgM_array\n",
    "            if '200c' in mass_type:\n",
    "                hmf = mass_function.massFunction(M_array, float(z_inference), mdef = '200c', model = 'tinker08', q_out = 'dndlnM')\n",
    "            if 'vir' in mass_type:\n",
    "                hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'vir', model = 'tinker08', q_out = 'dndlnM')    \n",
    "            if 'fof' in mass_type:\n",
    "                hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'fof', model = 'bhattacharya11', q_out = 'dndlnM')\n",
    "            lgM_rescaled = rescale_sub + (lgM_array - lgMmin)/(lgMmax-lgMmin)\n",
    "\n",
    "            int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
    "            hmf_pdf = hmf/int_val\n",
    "            # define the cdf of the halo mass function\n",
    "            hmf_cdf = np.zeros_like(hmf_pdf)\n",
    "            for i in range(len(hmf_cdf)):\n",
    "                hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n",
    "\n",
    "            if 'sigv' in config:\n",
    "                sigv = config['sigv']\n",
    "            else:\n",
    "                sigv = 0.05\n",
    "            num_cond_Ntot = num_cond\n",
    "            mu_all = np.arange(Nmax + 1) + 1\n",
    "            sig_all = sigv * np.ones_like(mu_all)\n",
    "            ngauss_Nhalo = Nmax + 1\n",
    "\n",
    "            model_BinaryMask = SumGaussModel(\n",
    "                hidden_dim=hidden_dim_MAF,\n",
    "                num_cond=num_cond_Ntot,\n",
    "                ngauss=2,\n",
    "                mu_all=mu_all[:2],\n",
    "                sig_all=sig_all[:2],\n",
    "                base_dist=base_dist_Ntot,\n",
    "                device=device_id\n",
    "                )\n",
    "\n",
    "            # model_BinaryMask.to(dev)\n",
    "\n",
    "\n",
    "            model_multiclass = SumGaussModel(\n",
    "                hidden_dim=hidden_dim_MAF,\n",
    "                num_cond=num_cond_Ntot,\n",
    "                ngauss=ngauss_Nhalo - 1,\n",
    "                mu_all=mu_all[1:] - 1,\n",
    "                sig_all=sig_all[1:],\n",
    "                base_dist=base_dist_Ntot,\n",
    "                device=device_id\n",
    "                )\n",
    "\n",
    "\n",
    "            # model_multiclass.to(dev)\n",
    "\n",
    "\n",
    "            num_cond_M1 = num_cond + 1\n",
    "            # # if conditioned on fastpm we will also give the fastpm fof M1 halos and its mask as conditional\n",
    "            # if cond_sim == 'fastpm':\n",
    "            #     num_cond_M1 += 2\n",
    "\n",
    "            model_M1 = NSF_1var_CNNcond(\n",
    "                K=K_M1,\n",
    "                B=B_M1,\n",
    "                hidden_dim=hidden_dim_MAF,\n",
    "                num_cond=num_cond_M1,\n",
    "                nflows=nflows_M1_NSF,\n",
    "                base_dist=base_dist_M1,\n",
    "                ngauss=ngauss_M1,\n",
    "                lgM_rs_tointerp=lgM_rescaled,\n",
    "                hmf_pdf_tointerp=hmf_pdf,\n",
    "                hmf_cdf_tointerp=hmf_cdf,\n",
    "                device=device_id \n",
    "                )\n",
    "\n",
    "            ndim_diff = Nmax - 1\n",
    "            num_cond_Mdiff = num_cond + 2\n",
    "            model_Mdiff = NSF_Autoreg_CNNcond(\n",
    "                dim=ndim_diff,\n",
    "                K=K_Mdiff,\n",
    "                B=B_Mdiff,\n",
    "                hidden_dim=hidden_dim_MAF,\n",
    "                num_cond=num_cond_Mdiff,\n",
    "                nflows=nflows_Mdiff_NSF,\n",
    "                base_dist=base_dist_Mdiff,\n",
    "                mu_pos=True\n",
    "                )\n",
    "\n",
    "\n",
    "            ndim = ndim_diff + 1\n",
    "            model_comb_mass = COMBINED_Model(\n",
    "                None,\n",
    "                model_Mdiff,\n",
    "                # None,\n",
    "                model_M1,\n",
    "                model_BinaryMask,\n",
    "                model_multiclass,\n",
    "                ndim,\n",
    "                ksize,\n",
    "                ns_d,\n",
    "                ns_h,\n",
    "                1,\n",
    "                ninp,\n",
    "                nfeature_cnn,\n",
    "                nout_cnn,\n",
    "                layers_types=layers_types,\n",
    "                act='tanh',\n",
    "                padding='valid',\n",
    "                sep_Binary_cond=True,\n",
    "                sep_MultiClass_cond=True,\n",
    "                sep_M1_cond=True,\n",
    "                sep_Mdiff_cond=True,\n",
    "                num_cond_Binary = num_cond_Ntot,\n",
    "                num_cond_MultiClass = num_cond_Ntot,\n",
    "                num_cond_M1 = num_cond_M1,\n",
    "                num_cond_Mdiff = num_cond_Mdiff\n",
    "                ).to(device_id)\n",
    "\n",
    "            # model = DDP(model, device_ids=[device_id], find_unused_parameters=True)\n",
    "\n",
    "            model_comb_mass = torch.nn.DataParallel(model_comb_mass)\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to load the mass model: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "\n",
    "            ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test0/'\n",
    "\n",
    "            checkpoint = torch.load(ldir_cp + f'test_model_bestfit_6484.pth', map_location=device_id)\n",
    "            # print(iter)\n",
    "            model_comb_mass.load_state_dict(checkpoint['state_dict'])\n",
    "            model_comb_mass.eval()\n",
    "\n",
    "            run_config_name = 'TRAIN_VEL_FREECOSMO_cond_fastpm_ns128_lresdata.yaml'\n",
    "            with open(\"/mnt/home/spandey/ceph/CHARM/run_configs/\" + run_config_name,\"r\") as file_object:\n",
    "                config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "            config_net = config['network_settings']\n",
    "            K_vel = config_net['K_vel']\n",
    "            B_vel = config_net['B_vel']\n",
    "            nflows_vel_NSF = config_net['nflows_vel_NSF']\n",
    "            cond_Mass_for_vel = config_net['cond_Mass_for_vel']\n",
    "            base_dist_vel = config_net['base_dist_vel']\n",
    "            ndim_mass = Nmax\n",
    "            ndim_vel = 3*Nmax\n",
    "            if cond_Mass_for_vel:\n",
    "                num_cond_vel = num_cond + ndim_mass\n",
    "            else:\n",
    "                num_cond_vel = num_cond\n",
    "\n",
    "            model_vel = NSF_Autoreg_CNNcond(\n",
    "                dim=ndim_vel,\n",
    "                K=K_vel,\n",
    "                B=B_vel,\n",
    "                hidden_dim=hidden_dim_MAF,\n",
    "                num_cond=num_cond_vel,\n",
    "                nflows=nflows_vel_NSF,\n",
    "                base_dist=base_dist_vel,\n",
    "                mu_pos=False\n",
    "                )\n",
    "\n",
    "\n",
    "            # ndim = ndim_diff + 1\n",
    "            model_comb_vel = COMBINED_Model_vel_only(\n",
    "                None,\n",
    "                model_vel,\n",
    "                ndim_vel,\n",
    "                ksize,\n",
    "                ns_d,\n",
    "                ns_h,\n",
    "                1,\n",
    "                ninp,\n",
    "                nfeature_cnn,\n",
    "                nout_cnn,\n",
    "                layers_types=layers_types,\n",
    "                act='tanh',\n",
    "                padding='valid',\n",
    "                ).to(device_id)\n",
    "\n",
    "\n",
    "\n",
    "            model_comb_vel = torch.nn.DataParallel(model_comb_vel)\n",
    "\n",
    "            ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test2_vel/'\n",
    "            # ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test0_vel/'\n",
    "\n",
    "            # checkpoint = torch.load(ldir_cp + f'test_model_bestfit_6600.pth', map_location=device_id)\n",
    "            checkpoint = torch.load(ldir_cp + f'test_model_bestfit_1000_evenmoredata_nsub11k.pth', map_location=device_id)            \n",
    "            # print(iter)\n",
    "            model_comb_vel.load_state_dict(checkpoint['state_dict'])\n",
    "            model_comb_vel.eval()\n",
    "\n",
    "\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to load the velocity model: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "\n",
    "\n",
    "\n",
    "            ldir_f = '/mnt/home/spandey/ceph/Quijote/data_NGP_self_fastpm_LH/'\n",
    "\n",
    "            n_dim_red = (nf - 1) // 2\n",
    "            n_pad = n_dim_red * nc\n",
    "\n",
    "            df_zg = pk.load(open(ldir_f + '%d/density_HR_full_m_res_128_z=0.5_nbatch_8_nfilter_3_ncnn_0.pk'%test_id,'rb'))\n",
    "            df_test_zg = df_zg['density_cic_unpad_combined']\n",
    "            df_test_pad_zg = np.pad(df_test_zg, n_pad, 'wrap')\n",
    "\n",
    "\n",
    "            z_REDSHIFT = float(z_all_FP[-1].split('_')[1])\n",
    "            if z_REDSHIFT == 0.0:\n",
    "                z_REDSHIFT = 0\n",
    "\n",
    "            df_load = pk.load(open(\n",
    "                ldir_f + '/' + str(test_id) + '/velocity_HR_full_m_res_128_z=' + str(z_REDSHIFT) + '_nbatch_8_nfilter_3_ncnn_0.pk', 'rb')\n",
    "                )\n",
    "\n",
    "            vel_all = df_load['velocity_cic_unpad_combined']\n",
    "            # # vel_pad_all = \n",
    "            vel_pad = np.stack([np.pad(vel_all[j,...], n_pad, 'wrap') for j in range(3)], axis=0)\n",
    "\n",
    "            df_test_all_pad = np.concatenate([np.log(1 + df_test_pad_zg + 1e-10)[None,...], vel_pad], axis=0)[None, None,:]\n",
    "            # df_test_all_pad = np.stack([np.log(1 + df_test_pad_zg + 1e-10), np.log(\n",
    "            #     1 + df_test_pad_zIC + 1e-10), df_test_pad_constrast_zg], axis=0)[None, None, :]\n",
    "\n",
    "            # df_test_all_pad.shape\n",
    "\n",
    "            # density_smoothed = gaussian_filter(df_test_zg, sigma=VALUE_SIG)\n",
    "            # df_test_constrast_zg = density_smoothed - df_test_zg\n",
    "            # \n",
    "            df_test_all_unpad = np.concatenate([np.log(1 + df_test_zg + 1e-10)[None,...], vel_all], axis=0)[None, None,:]\n",
    "            # df_test_all_unpad = np.stack([np.log(1 + df_test_zg + 1e-10), np.log(\n",
    "            #     1 + df_test_zIC + 1e-10), df_test_constrast_zg], axis=0)[None, None, :]\n",
    "\n",
    "            # df_test_all_unpad.shape\n",
    "\n",
    "            cond_nsh_test = np.moveaxis(df_test_all_unpad, 2, 5)\n",
    "            nsims_test = cond_nsh_test.shape[1]\n",
    "            nax_h_test = cond_nsh_test.shape[2]\n",
    "            ninp_test = cond_nsh_test.shape[-1]\n",
    "            cond_tensor_nsh_test = torch.Tensor(np.copy(cond_nsh_test.reshape(1,nsims_test * (nax_h_test ** 3), ninp_test))).to(device_id)    \n",
    "\n",
    "            # cond_tensor_nsh_test.shape\n",
    "            LH_cosmo_val_file='/mnt/home/spandey/ceph/Quijote/latin_hypercube_params.txt'\n",
    "            LH_cosmo_val_all = np.loadtxt(LH_cosmo_val_file)\n",
    "            fid_cosmo_val_all = LH_cosmo_val_all[test_id]\n",
    "            # fid_cosmo_val_all = np.array([0.3175, 0.049, 0.6711, 0.9624, 0.64])  \n",
    "\n",
    "            cosmo_val_test = np.tile(fid_cosmo_val_all, (cond_tensor_nsh_test.shape[1] ,1))[None,:]\n",
    "\n",
    "            # cosmo_val_test.shape\n",
    "            # df_test_all_pad.shape, df_test_all_unpad.shape, cosmo_val_test.shape\n",
    "            df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
    "            df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n",
    "            cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n",
    "\n",
    "\n",
    "            train_Ntot, train_M1, train_Mdiff = 1, 1, 1\n",
    "            train_binary, train_multi = 1, 1\n",
    "            # if verbose:\n",
    "                # print(f\"Running the model\")\n",
    "\n",
    "            # run the model\n",
    "            Ntot_samp_test, M1_samp_test, M_diff_samp_test, mask_tensor_M1_samp_test, mask_tensor_Mdiff_samp_test, _ = model_comb_mass.module.inverse(\n",
    "                cond_x=df_test_all_pad,\n",
    "                cond_x_nsh=df_test_all_unpad,\n",
    "                cond_cosmo=cosmo_val_test,\n",
    "                use_truth_Nhalo=1-train_Ntot,\n",
    "                use_truth_M1=1-train_M1,\n",
    "                use_truth_Mdiff=1-train_Mdiff,\n",
    "                mask_Mdiff_truth=None,\n",
    "                mask_M1_truth=None,\n",
    "                Nhalos_truth=None,\n",
    "                M1_truth=None,\n",
    "                Mdiff_truth=None,\n",
    "                train_binary=train_binary,\n",
    "                train_multi=train_multi,\n",
    "                train_M1=train_M1,\n",
    "                train_Mdiff=train_Mdiff,\n",
    "            )\n",
    "\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to sample from the mass model: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "\n",
    "            BoxSize=1000\n",
    "            # Ntot_samp_test[0].shape\n",
    "            Ntot_samp_test_rs = Ntot_samp_test[0][:, np.newaxis]\n",
    "            M1_samp_test_rs = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:, 0]).cpu().detach().numpy()\n",
    "            M_diff_samp_test_rs = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            Ntot_samp_test = Ntot_samp_test[0][:, np.newaxis]\n",
    "            save_subvol_Nhalo = Ntot_samp_test.reshape(\n",
    "                nsims_test, nax_h_test, nax_h_test, nax_h_test)\n",
    "            save_subvol_M1 = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:, 0]\n",
    "                                ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, 1)\n",
    "            save_subvol_Mdiff = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]\n",
    "                                    ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff)\n",
    "\n",
    "            mask_subvol_Mtot1 = mask_tensor_M1_samp_test[0].cpu().detach().numpy().reshape(\n",
    "                nsims_test, nax_h_test, nax_h_test, nax_h_test)[..., None]\n",
    "            mask_subvol_Mtot2 = mask_tensor_Mdiff_samp_test[0].cpu().detach().numpy(\n",
    "            ).reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff)\n",
    "            mask_subvol_Mtot = np.concatenate(\n",
    "                [mask_subvol_Mtot1, mask_subvol_Mtot2], axis=-1)\n",
    "\n",
    "            # compute the mass of halos from output\n",
    "            save_subvol_Mtot = np.zeros(\n",
    "                (nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff + 1))\n",
    "            # Mmin, Mmax = return_dict_test['Mmin'], return_dict_test['Mmax']\n",
    "            for jd in range(ndim_diff + 1):\n",
    "                if jd == 0:\n",
    "                    save_subvol_Mtot[..., jd] = (\n",
    "                        save_subvol_M1[..., 0] + 0.5) * (lgMmax - lgMmin) + lgMmin\n",
    "                else:\n",
    "                    save_subvol_Mtot[...,\n",
    "                                        jd] = (save_subvol_Mtot[..., jd - 1]) - (save_subvol_Mdiff[..., jd - 1]) * (lgMmax - lgMmin)\n",
    "\n",
    "            save_subvol_Mtot *= mask_subvol_Mtot\n",
    "\n",
    "            Nhalos = save_subvol_Nhalo[0, ...]  # histogram of halos in each voxel\n",
    "            M_halos = save_subvol_Mtot[0, ...]  # mass of halos in each voxel\n",
    "            M_halos_sort_norm = rescale_sub + (M_halos - lgMmin)/(lgMmax - lgMmin)\n",
    "            M_halos_sort_norm *= mask_subvol_Mtot[0, ...]\n",
    "            M_halos_sort_norm_condvel = M_halos_sort_norm.reshape(nax_h_test**3, -1)\n",
    "            # create the meshgrid\n",
    "            xall = (np.linspace(0, BoxSize, ns_h + 1))\n",
    "            xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "            yarray = np.copy(xarray)\n",
    "            zarray = np.copy(xarray)\n",
    "            x_cy, y_cy, z_cy = np.meshgrid(xarray, yarray, zarray, indexing='ij')\n",
    "\n",
    "            # record discrete halo positions and masses\n",
    "            x_h_mock, y_h_mock, z_h_mock, lgM_mock = [], [], [], []\n",
    "            # Nmax_sel = 3\n",
    "            k = 0\n",
    "            for jx in range(ns_h):\n",
    "                for jy in range(ns_h):\n",
    "                    for jz in range(ns_h):\n",
    "                        Nh_vox = int(Nhalos[jx, jy, jz])\n",
    "                        if Nh_vox > 0:\n",
    "                            x_h_mock.append(x_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                            y_h_mock.append(y_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                            z_h_mock.append(z_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "\n",
    "                            lgM_mock.append((M_halos[jx, jy, jz, :Nh_vox]))\n",
    "                            k += Nh_vox\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            x_h_mock = np.concatenate(x_h_mock)\n",
    "            y_h_mock = np.concatenate(y_h_mock)\n",
    "            z_h_mock = np.concatenate(z_h_mock)\n",
    "            pos_h_mock = np.vstack((x_h_mock, y_h_mock, z_h_mock)).T\n",
    "            lgMass_mock = np.concatenate(lgM_mock)\n",
    "            # convert to float data type\n",
    "            pos_h_mock = pos_h_mock.astype('float32')\n",
    "            lgMass_mock = lgMass_mock.astype('float32')\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to process masses/numbers: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "            \n",
    "\n",
    "            df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
    "            df_test_all_unpad = torch.tensor(df_test_all_unpad).to(device_id)\n",
    "            cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n",
    "            Nhalos_truth_recomb_tensor = torch.Tensor(Ntot_samp_test_rs[None,...]).to(device_id)\n",
    "            if cond_Mass_for_vel:\n",
    "                Mhalos_truth_recomb_tensor = torch.Tensor(M_halos_sort_norm_condvel[None,...]).to(device_id)\n",
    "            else:\n",
    "                Mhalos_truth_recomb_tensor = None\n",
    "\n",
    "            vel_samp_out = model_comb_vel.module.inverse(cond_x=df_test_all_pad,\n",
    "                                        cond_x_nsh=df_test_all_unpad,\n",
    "                                        cond_cosmo=cosmo_val_test,\n",
    "                                        mask_vel_truth=None,\n",
    "                                        Nhalos_truth=Nhalos_truth_recomb_tensor,\n",
    "                                        Mhalos_truth=Mhalos_truth_recomb_tensor,\n",
    "                                        # Mhalos_truth=None,                            \n",
    "                                        vel_truth=None)\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to sample from the velocity model: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            import pickle as pk\n",
    "            from scipy.interpolate import RegularGridInterpolator\n",
    "            grid = 128\n",
    "            BoxSize = 1000.\n",
    "            xall = (np.linspace(0, BoxSize, grid + 1))\n",
    "            xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "            yarray = np.copy(xarray)\n",
    "            zarray = np.copy(xarray)\n",
    "\n",
    "            vel_load_dir = '/mnt/home/spandey/ceph/Quijote/data_NGP_self_fastpm_LH/'\n",
    "            df = pk.load(open(vel_load_dir + f'{test_id}/velocity_HR_full_m_res_128_z=0.5_nbatch_8_nfilter_3_ncnn_0.pk', 'rb'))['velocity_cic_unpad_combined']\n",
    "            # df.shape\n",
    "            vx_mesh_load = 1000.*df[0,...]\n",
    "            vy_mesh_load = 1000.*df[1,...]\n",
    "            vz_mesh_load = 1000.*df[2,...]\n",
    "\n",
    "            vx_all_3D_interp_l = RegularGridInterpolator((xarray, yarray, zarray), vx_mesh_load, bounds_error=False, fill_value=None)\n",
    "            vy_all_3D_interp_l = RegularGridInterpolator((xarray, yarray, zarray), vy_mesh_load, bounds_error=False, fill_value=None)\n",
    "            vz_all_3D_interp_l = RegularGridInterpolator((xarray, yarray, zarray), vz_mesh_load, bounds_error=False, fill_value=None)\n",
    "\n",
    "            vx_eval_interp_l = vx_all_3D_interp_l(pos_h_mock)\n",
    "            vy_eval_interp_l = vy_all_3D_interp_l(pos_h_mock)\n",
    "            vz_eval_interp_l = vz_all_3D_interp_l(pos_h_mock)\n",
    "\n",
    "            vmax = 1000\n",
    "            vmin = -1000\n",
    "            jsim = test_id\n",
    "            ndim_diff = Nmax - 1\n",
    "            sdir_fid='/mnt/home/spandey/ceph/Quijote/data_NGP_self_LH'\n",
    "            mass_type = 'rockstar_200c'\n",
    "            # try:\n",
    "            fname = sdir_fid + '/' + str(jsim) + '/SPARSEMATS_halos_HR_rockstar_200c_lgMmincut_5e12_full_res_128_z=0.5.pk'\n",
    "            df_h_full = pk.load(open(fname, 'rb'))\n",
    "\n",
    "            df_Mh_all_truth_comb, df_Nh_truth_comb = df_h_full['M_halos_combined'].todense(), df_h_full['N_halos_combined'].todense()\n",
    "            v_halos_pred_recomb = df_h_full['v_halos_pred_combined'].todense()\n",
    "            v_halos_truth_recomb = df_h_full['v_halos_true_combined'].todense()\n",
    "\n",
    "            Nhalos_pred_recomb = Ntot_samp_test_rs[...,0].reshape(ns_h, ns_h, ns_h)\n",
    "\n",
    "            Nhalos_truth_recomb = np.clip(df_Nh_truth_comb, 0, Nmax)\n",
    "            M_halos_truth_recomb = df_Mh_all_truth_comb[...,:Nmax]\n",
    "\n",
    "\n",
    "            # v_halos_diff_recomb = np.reshape(vel_samp_out, (1, 128, 128, 128, ndim_diff + 1))[0,...]\n",
    "            v_halos_diff_recomb = np.reshape(vel_samp_out, (1, 128, 128, 128, (ndim_diff + 1)*3))[0,...]\n",
    "            v_halos_diff_recomb = np.reshape(v_halos_diff_recomb, (128, 128, 128, (ndim_diff + 1), 3))\n",
    "\n",
    "\n",
    "            ind_cart_lin = {}\n",
    "            ind_lin_cart = {}\n",
    "            jc = 0\n",
    "            # from tqdm import tqdm\n",
    "            for jx in (range(nb)):\n",
    "                for jy in range(nb):\n",
    "                    for jz in range(nb):\n",
    "                        # get the sub-cube\n",
    "                        ind_lin_cart[(jx, jy, jz)] = jc\n",
    "                        ind_cart_lin[jc] = [jx, jy, jz]\n",
    "                        jc += 1\n",
    "\n",
    "            xall = (np.linspace(0, 1000, ns_h + 1))\n",
    "            xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "            yarray = np.copy(xarray)\n",
    "            zarray = np.copy(xarray)\n",
    "            x_cy, y_cy, z_cy = np.meshgrid(xarray, yarray, zarray, indexing='ij')\n",
    "\n",
    "\n",
    "            x_h_truth, y_h_truth, z_h_truth, lgM_truth,  = [], [], [], []\n",
    "            vx_truth_orig, vx_pred_orig = [], []\n",
    "            vx_diff_orig = []\n",
    "            vx_wodiff_orig = []\n",
    "\n",
    "            vy_truth_orig, vy_pred_orig = [], []\n",
    "            vy_diff_orig = []\n",
    "            vy_wodiff_orig = []\n",
    "\n",
    "            vz_truth_orig, vz_pred_orig = [], []\n",
    "            vz_diff_orig = []\n",
    "            vz_wodiff_orig = []\n",
    "\n",
    "            for jx in range(ns_h):\n",
    "                for jy in range(ns_h):\n",
    "                    for jz in range(ns_h):\n",
    "                            Nh_vox = int(Nhalos_truth_recomb[jx, jy, jz])\n",
    "                            # Nh_vox = int(df_Nh_truth_comb[jx, jy, jz])                    \n",
    "                            if Nh_vox > 0:\n",
    "                                # if Nh_vox > Nmax_sel:\n",
    "                                #     Nh_vox = Nmax_sel\n",
    "\n",
    "                                x_h_truth.append(x_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                                y_h_truth.append(y_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                                z_h_truth.append(z_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                                \n",
    "                                lgM_truth.append((M_halos_truth_recomb[jx, jy, jz, :Nh_vox]))\n",
    "\n",
    "                                vx_truth_orig.append((v_halos_truth_recomb[...,0][jx, jy, jz, :Nh_vox]))\n",
    "                                vx_diff_orig.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 0])*((vmax - vmin)) + vmin))\n",
    "                                vx_pred_here = (v_halos_pred_recomb[...,0][jx, jy, jz, :Nh_vox]) - ((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 0])*((vmax - vmin)) + vmin)\n",
    "                                vx_pred_orig.append((vx_pred_here))\n",
    "                                vx_wodiff_orig.append(v_halos_pred_recomb[...,0][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "\n",
    "                                vy_truth_orig.append((v_halos_truth_recomb[...,1][jx, jy, jz, :Nh_vox]))\n",
    "                                vy_diff_orig.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 1])*((vmax - vmin)) + vmin))\n",
    "                                vy_pred_here = (v_halos_pred_recomb[...,1][jx, jy, jz, :Nh_vox]) - ((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 1])*((vmax - vmin)) + vmin)\n",
    "                                vy_pred_orig.append((vy_pred_here))\n",
    "                                vy_wodiff_orig.append(v_halos_pred_recomb[...,1][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "                                vz_truth_orig.append((v_halos_truth_recomb[...,2][jx, jy, jz, :Nh_vox]))\n",
    "                                vz_diff_orig.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 2])*((vmax - vmin)) + vmin))\n",
    "                                vz_pred_here = (v_halos_pred_recomb[...,2][jx, jy, jz, :Nh_vox]) - ((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 2])*((vmax - vmin)) + vmin)\n",
    "                                vz_pred_orig.append((vz_pred_here))\n",
    "                                vz_wodiff_orig.append(v_halos_pred_recomb[...,2][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # x_h_truth, y_h_truth, z_h_truth, lgM_truth, vx_truth_orig, vx_pred_orig = [], [], [], [], [], []\n",
    "            vx_diff_mock = []\n",
    "            # vx_wodiff_orig = []\n",
    "\n",
    "            vy_diff_mock = []\n",
    "            # vy_wodiff_orig = []\n",
    "\n",
    "            vz_diff_mock = []\n",
    "            # vz_wodiff_orig = []\n",
    "\n",
    "\n",
    "            for jx in range(ns_h):\n",
    "                for jy in range(ns_h):\n",
    "                    for jz in range(ns_h):\n",
    "                            Nh_vox = int(Nhalos_pred_recomb[jx, jy, jz])\n",
    "                            # Nh_vox = int(df_Nh_truth_comb[jx, jy, jz])                    \n",
    "                            if Nh_vox > 0:\n",
    "                                vx_diff_mock.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 0])*((vmax - vmin)) + vmin))\n",
    "\n",
    "                                vy_diff_mock.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 1])*((vmax - vmin)) + vmin))\n",
    "\n",
    "                                vz_diff_mock.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 2])*((vmax - vmin)) + vmin))\n",
    "\n",
    "                                # vx_wodiff_orig.append(v_halos_pred_recomb[...,0][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "\n",
    "            # vx_eval_interp_l = vx_all_3D_interp_l(pos_h_mock)\n",
    "            # pos_h_mock.shape\n",
    "            vx_total_mock = vx_eval_interp_l - np.concatenate(vx_diff_mock)\n",
    "\n",
    "            vy_total_mock = vy_eval_interp_l - np.concatenate(vy_diff_mock)\n",
    "\n",
    "            vz_total_mock = vz_eval_interp_l - np.concatenate(vz_diff_mock)\n",
    "\n",
    "            vel_h_mock = np.vstack((vx_total_mock, vy_total_mock, vz_total_mock)).T\n",
    "\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to process velocities: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            isim = test_id\n",
    "            # snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/fiducial_HR'\n",
    "            snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/latin_hypercube_HR'\n",
    "            snapnum = 3\n",
    "            # MAS     = 'NGP'  #mass-assigment scheme\n",
    "            # verbose = False   #print information on progress\n",
    "            snapdir = snap_dir_base + '/' + str(isim)  #folder hosting the catalogue\n",
    "            rockstar = np.loadtxt(snapdir + '/out_' + str(snapnum) + '_pid.list')\n",
    "            with open(snapdir + '/out_' + str(snapnum) + '_pid.list', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to open and read true file: {tf - ti:.2f}\")\n",
    "            ti = time()            \n",
    "            header = lines[0].split()\n",
    "            # get the properties of the halos\n",
    "            pos_h_truth = rockstar[:,header.index('X'):header.index('Z')+1]\n",
    "            index_M = header.index('M200c')                    \n",
    "            mass_truth = rockstar[:,index_M]  #Halo masses in Msun/h\n",
    "            lgMass_truth = np.log10(mass_truth).astype(np.float32)\n",
    "            vel_h_truth = rockstar[:,header.index('VX'):header.index('VZ')+1]\n",
    "\n",
    "            indsel = np.where(mass_truth > 5e12)[0]\n",
    "            pos_h_truth = pos_h_truth[indsel]\n",
    "            vel_h_truth = vel_h_truth[indsel]\n",
    "            lgMass_truth = lgMass_truth[indsel]\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to load the truth halos: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "\n",
    "            import redshift_space_library as RSL\n",
    "            BoxSize = 1000.\n",
    "            Hubble = fid_cosmo_val_all[2]*100.\n",
    "            redshift = 0.5\n",
    "\n",
    "            axis = 0\n",
    "            velx_orig = (vel_h_truth[:,0]).astype(np.float32)[:, None]\n",
    "            velx_charm = (vx_total_mock).astype(np.float32)[:, None]\n",
    "            pos_rsdx_orig = np.copy(pos_h_truth).astype(np.float32)\n",
    "            pos_rsdx_charm = np.copy(np.ascontiguousarray(pos_h_mock)).astype(np.float32)\n",
    "            RSL.pos_redshift_space(pos_rsdx_orig, velx_orig, BoxSize, Hubble, redshift, axis)\n",
    "            RSL.pos_redshift_space(pos_rsdx_charm, velx_charm, BoxSize, Hubble, redshift, axis)\n",
    "\n",
    "            axis = 1\n",
    "            velx_orig = (vel_h_truth[:,1]).astype(np.float32)[:, None]\n",
    "            velx_charm = (vy_total_mock).astype(np.float32)[:, None]\n",
    "            pos_rsdy_orig = np.copy(pos_h_truth).astype(np.float32)\n",
    "            pos_rsdy_charm = np.copy(np.ascontiguousarray(pos_h_mock)).astype(np.float32)\n",
    "            RSL.pos_redshift_space(pos_rsdy_orig, velx_orig, BoxSize, Hubble, redshift, axis)\n",
    "            RSL.pos_redshift_space(pos_rsdy_charm, velx_charm, BoxSize, Hubble, redshift, axis)\n",
    "\n",
    "            axis = 2\n",
    "            velx_orig = (vel_h_truth[:,2]).astype(np.float32)[:, None]\n",
    "            velx_charm = (vz_total_mock).astype(np.float32)[:, None]\n",
    "            pos_rsdz_orig = np.copy(pos_h_truth).astype(np.float32)\n",
    "            pos_rsdz_charm = np.copy(np.ascontiguousarray(pos_h_mock)).astype(np.float32)\n",
    "            RSL.pos_redshift_space(pos_rsdz_orig, velx_orig, BoxSize, Hubble, redshift, axis)\n",
    "            RSL.pos_redshift_space(pos_rsdz_charm, velx_charm, BoxSize, Hubble, redshift, axis)\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to compute RSD: {tf - ti:.2f}\")\n",
    "            ti = time()\n",
    "\n",
    "            saved = {'lgmass_truth': lgMass_truth, 'pos_truth': pos_h_truth, 'vel_truth': vel_h_truth, \n",
    "                    'lgmass_mock': lgMass_mock, 'pos_mock': pos_h_mock, 'vel_mock': vel_h_mock,\n",
    "                    'pos_rsdx_truth': pos_rsdx_orig, 'pos_rsdx_mock': pos_rsdx_charm,\n",
    "                    'pos_rsdy_truth': pos_rsdy_orig, 'pos_rsdy_mock': pos_rsdy_charm,\n",
    "                    'pos_rsdz_truth': pos_rsdz_orig, 'pos_rsdz_mock': pos_rsdz_charm\n",
    "                    }\n",
    "\n",
    "            import pickle as pk\n",
    "            pk.dump(saved, open(save_fname_cats, 'wb'))\n",
    "            tf = time()\n",
    "            if verbose:\n",
    "                print(f\"Time to save the halo cats: {tf - ti:.2f}\")\n",
    "                print(f\"Total time: {tf - t0:.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in test_id: {test_id}\")\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH:  1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828430/4000756230.py:160: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
      "/tmp/ipykernel_2828430/4000756230.py:165: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the mass model: 0.51\n",
      "Time to load the velocity model: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828430/4000756230.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n",
      "/mnt/home/spandey/miniconda3/envs/ili-sbi/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the mass model: 2.34\n",
      "Time to process masses/numbers: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828430/4000756230.py:515: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
      "/tmp/ipykernel_2828430/4000756230.py:516: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(df_test_all_unpad).to(device_id)\n",
      "/tmp/ipykernel_2828430/4000756230.py:517: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the velocity model: 0.78\n",
      "Time to process velocities: 3.59\n",
      "Time to open and read true file: 1.46\n",
      "Time to load the truth halos: 0.02\n",
      "Time to compute RSD: 0.01\n",
      "Time to save the halo cats: 0.04\n",
      "Total time: 10.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828430/4000756230.py:720: RuntimeWarning: divide by zero encountered in log10\n",
      "  lgMass_truth = np.log10(mass_truth).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH:  1797\n",
      "Time to load the mass model: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828430/4000756230.py:160: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
      "/tmp/ipykernel_2828430/4000756230.py:165: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the velocity model: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828430/4000756230.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the mass model: 1.43\n",
      "Time to process masses/numbers: 2.21\n",
      "Time to sample from the velocity model: 0.98\n",
      "Time to process velocities: 6.61\n",
      "Time to open and read true file: 2.04\n",
      "Time to load the truth halos: 0.03\n",
      "Time to compute RSD: 0.01\n",
      "Time to save the halo cats: 0.08\n",
      "Total time: 13.87\n",
      "LH:  1860\n",
      "Time to load the mass model: 0.03\n",
      "Time to load the velocity model: 0.35\n",
      "Time to sample from the mass model: 1.27\n",
      "Time to process masses/numbers: 2.61\n",
      "Time to sample from the velocity model: 1.07\n",
      "Time to process velocities: 8.51\n",
      "Time to open and read true file: 2.31\n",
      "Time to load the truth halos: 0.04\n",
      "Time to compute RSD: 0.01\n",
      "Time to save the halo cats: 0.11\n",
      "Total time: 16.31\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    # n1 = int(sys.argv[1])\n",
    "    # n2 = int(sys.argv[2])\n",
    "for test_id in (range(0, 2000)):\n",
    "#     save_cats(test_id)\n",
    "# test_id = int(sys.argv[1])\n",
    "    save_cats(test_id, verbose=True)\n",
    "    # for test_id in (range(361, 362)):\n",
    "    #     save_cats(test_id, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH:  112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1988540/1149007865.py:114: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
      "/tmp/ipykernel_1988540/1149007865.py:119: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the mass model: 0.20\n",
      "Time to load the velocity model: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1988540/1149007865.py:354: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the mass model: 2.91\n",
      "Time to process masses/numbers: 7.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1988540/1149007865.py:468: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
      "/tmp/ipykernel_1988540/1149007865.py:469: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(df_test_all_unpad).to(device_id)\n",
      "/tmp/ipykernel_1988540/1149007865.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the velocity model: 2.14\n",
      "Time to process velocities: 26.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1988540/1149007865.py:669: RuntimeWarning: divide by zero encountered in log10\n",
      "  lgMass_truth = np.log10(mass_truth).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the truth halos: 3.62\n",
      "Time to compute RSD: 0.08\n",
      "Time to save the halo cats: 0.41\n",
      "Total time: 42.97\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # n1 = int(sys.argv[1])\n",
    "#     # n2 = int(sys.argv[2])\n",
    "#     # for test_id in (range(n1, n2)):\n",
    "#     #     save_cats(test_id)\n",
    "#     test_id = int(sys.argv[1])\n",
    "#     save_cats(test_id)\n",
    "#     # for test_id in (range(361, 362)):\n",
    "#     #     save_cats(test_id, verbose=True)\n",
    "\n",
    "save_cats(112, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1988540/3594633440.py:16: RuntimeWarning: divide by zero encountered in log10\n",
      "  lgMass_truth = np.log10(mass_truth).astype(np.float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'verbose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m lgMass_truth \u001b[38;5;241m=\u001b[39m lgMass_truth[indsel]\n\u001b[1;32m     23\u001b[0m tf \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mverbose\u001b[49m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to load the truth halos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mti\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m ti \u001b[38;5;241m=\u001b[39m time()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'verbose' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "isim = 112\n",
    "# snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/fiducial_HR'\n",
    "snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/latin_hypercube_HR'\n",
    "snapnum = 3\n",
    "# MAS     = 'NGP'  #mass-assigment scheme\n",
    "# verbose = False   #print information on progress\n",
    "snapdir = snap_dir_base + '/' + str(isim)  #folder hosting the catalogue\n",
    "rockstar = np.loadtxt(snapdir + '/out_' + str(snapnum) + '_pid.list')\n",
    "with open(snapdir + '/out_' + str(snapnum) + '_pid.list', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "header = lines[0].split()\n",
    "# get the properties of the halos\n",
    "pos_h_truth = rockstar[:,header.index('X'):header.index('Z')+1]\n",
    "index_M = header.index('M200c')                    \n",
    "mass_truth = rockstar[:,index_M]  #Halo masses in Msun/h\n",
    "lgMass_truth = np.log10(mass_truth).astype(np.float32)\n",
    "vel_h_truth = rockstar[:,header.index('VX'):header.index('VZ')+1]\n",
    "\n",
    "indsel = np.where(mass_truth > 5e12)[0]\n",
    "pos_h_truth = pos_h_truth[indsel]\n",
    "vel_h_truth = vel_h_truth[indsel]\n",
    "lgMass_truth = lgMass_truth[indsel]\n",
    "tf = time()\n",
    "if verbose:\n",
    "    print(f\"Time to load the truth halos: {tf - ti:.2f}\")\n",
    "ti = time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH:  1611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2338760/4151508983.py:160: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
      "/tmp/ipykernel_2338760/4151508983.py:165: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the mass model: 0.23\n",
      "Time to load the velocity model: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2338760/4151508983.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the mass model: 2.89\n",
      "Time to process masses/numbers: 6.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2338760/4151508983.py:514: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
      "/tmp/ipykernel_2338760/4151508983.py:515: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(df_test_all_unpad).to(device_id)\n",
      "/tmp/ipykernel_2338760/4151508983.py:516: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sample from the velocity model: 1.96\n",
      "Time to process velocities: 22.73\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from time import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import pickle as pk\n",
    "import pathlib\n",
    "curr_path = pathlib.Path().absolute()\n",
    "src_path = os.path.abspath(curr_path / \"../charm/\") \n",
    "sys.path.append(src_path)\n",
    "# from charm import SumGaussModel, COMBINED_Model, NSF_1var_CNNcond, NSF_Autoreg_CNNcond, COMBINED_Model_vel_only\n",
    "from all_models import SumGaussModel, NSF_1var_CNNcond, NSF_Autoreg_CNNcond\n",
    "from combined_models import COMBINED_Model, COMBINED_Model_vel_only\n",
    "# from utils_data_prep_cosmo_vel import *\n",
    "from colossus.cosmology import cosmology\n",
    "params = {'flat': True, 'H0': 67.11, 'Om0': 0.3175, 'Ob0': 0.049, 'sigma8': 0.834, 'ns': 0.9624}\n",
    "cosmo = cosmology.setCosmology('myCosmo', **params)\n",
    "from colossus.lss import mass_function\n",
    "from tqdm import tqdm\n",
    "import sparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import matplotlib.pyplot as pl\n",
    "import scipy as sp\n",
    "import os\n",
    "\n",
    "test_id = 1611\n",
    "verbose = True\n",
    "\n",
    "import pickle as pk\n",
    "save_fname_cats = f'/mnt/home/spandey/ceph/CHARM/data/halo_cats_charm_truth/halo_cat_pos_vel_LH_{test_id}.pk'\n",
    "# if file does not exist:\n",
    "if not os.path.exists(save_fname_cats):\n",
    "\n",
    "    print('LH: ', test_id)\n",
    "\n",
    "    ti = time()\n",
    "    t0 = time()\n",
    "    run_config_name = 'TRAIN_MASS_FREECOSMO_cond_fastpm_ns128.yaml'\n",
    "    with open(\"/mnt/home/spandey/ceph/CHARM/run_configs/\" + run_config_name,\"r\") as file_object:\n",
    "        config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "    config_sims = config['sim_settings']\n",
    "    ji_array = np.arange(int(config_sims['nsims']))\n",
    "    nsubvol_per_ji = int(config_sims['nsubvol_per_ji'])\n",
    "    nsubvol_fid = int(config_sims['nsubvol_fid'])\n",
    "    subsel_criteria = config_sims['subsel_criteria']\n",
    "    num_cosmo_params = int(config_sims['num_cosmo_params'])\n",
    "    ns_d = config_sims['ns_d']\n",
    "    nb = config_sims['nb']\n",
    "    nax_d =  ns_d // nb\n",
    "    nf = config_sims['nf']\n",
    "    layers_types = config_sims['layers_types']\n",
    "    z_inference = config_sims['z_inference']\n",
    "    nc = 0\n",
    "    for jl in range(len(layers_types)):\n",
    "        if layers_types[jl] == 'cnn':\n",
    "            nc += 1\n",
    "        elif layers_types[jl] == 'res':\n",
    "            nc += 2\n",
    "        else:\n",
    "            raise ValueError(\"layer type not supported\")\n",
    "\n",
    "    z_all = config_sims['z_all']\n",
    "    z_all_FP = config_sims['z_all_FP']\n",
    "    # z_all_FP = z_all_FP[:-1]\n",
    "    z_all_FP = z_all_FP\n",
    "    ns_h = config_sims['ns_h']\n",
    "    nax_h = ns_h // nb\n",
    "    cond_sim = config_sims['cond_sim']\n",
    "\n",
    "    nsims_per_batch = config_sims['nsims_per_batch']\n",
    "    nbatches_train = config_sims['nbatches_train']\n",
    "\n",
    "    mass_type = config_sims['mass_type']\n",
    "    lgMmin = config_sims['lgMmin']\n",
    "    lgMmax = config_sims['lgMmax']\n",
    "    stype = config_sims['stype']\n",
    "    rescale_sub = config_sims['rescale_sub']\n",
    "    lgMmincutstr = config_sims['lgMmincutstr']\n",
    "    # subsel_highM1 = config_sims['subsel_highM1']\n",
    "    # nsubsel = config_sims['nsubsel']\n",
    "    is_HR = config_sims['is_HR']\n",
    "\n",
    "    try:\n",
    "        Nmax = config_sims['Nmax']\n",
    "    except:\n",
    "        Nmax = 4\n",
    "\n",
    "    config_net = config['network_settings']\n",
    "    hidden_dim_MAF = config_net['hidden_dim_MAF']\n",
    "    learning_rate = config_net['learning_rate']\n",
    "    K_M1 = config_net['K_M1']\n",
    "    B_M1 = config_net['B_M1']\n",
    "    nflows_M1_NSF = config_net['nflows_M1_NSF']\n",
    "\n",
    "    K_Mdiff = config_net['K_Mdiff']\n",
    "    B_Mdiff = config_net['B_Mdiff']\n",
    "    nflows_Mdiff_NSF = config_net['nflows_Mdiff_NSF']\n",
    "\n",
    "    base_dist_Ntot = config_net['base_dist_Ntot']\n",
    "    if base_dist_Ntot == 'None':\n",
    "        base_dist_Ntot = None\n",
    "    base_dist_M1 = config_net['base_dist_M1']\n",
    "    base_dist_Mdiff = config_net['base_dist_Mdiff']\n",
    "    ngauss_M1 = config_net['ngauss_M1']\n",
    "\n",
    "    changelr = config_net['changelr']\n",
    "    ksize = nf\n",
    "    nfeature_cnn = config_net['nfeature_cnn']\n",
    "    nout_cnn = 4 * nfeature_cnn\n",
    "    if cond_sim == 'fastpm':\n",
    "        if any('v' in str(string) for string in z_all_FP):\n",
    "            ninp = len(z_all_FP) + 2\n",
    "        else:\n",
    "            ninp = len(z_all_FP)\n",
    "\n",
    "    elif cond_sim == 'quijote':\n",
    "        ninp = len(z_all)\n",
    "    else:\n",
    "        raise ValueError(\"cond_sim not supported\")\n",
    "\n",
    "    num_cond = nout_cnn + ninp + num_cosmo_params\n",
    "\n",
    "\n",
    "    device_id = torch.device(\"cuda\")\n",
    "    # device_id = torch.device(\"cpu\")    \n",
    "    ndim_diff = Nmax - 1\n",
    "\n",
    "    lgM_array = np.linspace(lgMmin, lgMmax, 100)\n",
    "    M_array = 10**lgM_array\n",
    "    if '200c' in mass_type:\n",
    "        hmf = mass_function.massFunction(M_array, float(z_inference), mdef = '200c', model = 'tinker08', q_out = 'dndlnM')\n",
    "    if 'vir' in mass_type:\n",
    "        hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'vir', model = 'tinker08', q_out = 'dndlnM')    \n",
    "    if 'fof' in mass_type:\n",
    "        hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'fof', model = 'bhattacharya11', q_out = 'dndlnM')\n",
    "    lgM_rescaled = rescale_sub + (lgM_array - lgMmin)/(lgMmax-lgMmin)\n",
    "\n",
    "    int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
    "    hmf_pdf = hmf/int_val\n",
    "    # define the cdf of the halo mass function\n",
    "    hmf_cdf = np.zeros_like(hmf_pdf)\n",
    "    for i in range(len(hmf_cdf)):\n",
    "        hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n",
    "\n",
    "    if 'sigv' in config:\n",
    "        sigv = config['sigv']\n",
    "    else:\n",
    "        sigv = 0.05\n",
    "    num_cond_Ntot = num_cond\n",
    "    mu_all = np.arange(Nmax + 1) + 1\n",
    "    sig_all = sigv * np.ones_like(mu_all)\n",
    "    ngauss_Nhalo = Nmax + 1\n",
    "\n",
    "    model_BinaryMask = SumGaussModel(\n",
    "        hidden_dim=hidden_dim_MAF,\n",
    "        num_cond=num_cond_Ntot,\n",
    "        ngauss=2,\n",
    "        mu_all=mu_all[:2],\n",
    "        sig_all=sig_all[:2],\n",
    "        base_dist=base_dist_Ntot,\n",
    "        device=device_id\n",
    "        )\n",
    "\n",
    "    # model_BinaryMask.to(dev)\n",
    "\n",
    "\n",
    "    model_multiclass = SumGaussModel(\n",
    "        hidden_dim=hidden_dim_MAF,\n",
    "        num_cond=num_cond_Ntot,\n",
    "        ngauss=ngauss_Nhalo - 1,\n",
    "        mu_all=mu_all[1:] - 1,\n",
    "        sig_all=sig_all[1:],\n",
    "        base_dist=base_dist_Ntot,\n",
    "        device=device_id\n",
    "        )\n",
    "\n",
    "\n",
    "    # model_multiclass.to(dev)\n",
    "\n",
    "\n",
    "    num_cond_M1 = num_cond + 1\n",
    "    # # if conditioned on fastpm we will also give the fastpm fof M1 halos and its mask as conditional\n",
    "    # if cond_sim == 'fastpm':\n",
    "    #     num_cond_M1 += 2\n",
    "\n",
    "    model_M1 = NSF_1var_CNNcond(\n",
    "        K=K_M1,\n",
    "        B=B_M1,\n",
    "        hidden_dim=hidden_dim_MAF,\n",
    "        num_cond=num_cond_M1,\n",
    "        nflows=nflows_M1_NSF,\n",
    "        base_dist=base_dist_M1,\n",
    "        ngauss=ngauss_M1,\n",
    "        lgM_rs_tointerp=lgM_rescaled,\n",
    "        hmf_pdf_tointerp=hmf_pdf,\n",
    "        hmf_cdf_tointerp=hmf_cdf,\n",
    "        device=device_id \n",
    "        )\n",
    "\n",
    "    ndim_diff = Nmax - 1\n",
    "    num_cond_Mdiff = num_cond + 2\n",
    "    model_Mdiff = NSF_Autoreg_CNNcond(\n",
    "        dim=ndim_diff,\n",
    "        K=K_Mdiff,\n",
    "        B=B_Mdiff,\n",
    "        hidden_dim=hidden_dim_MAF,\n",
    "        num_cond=num_cond_Mdiff,\n",
    "        nflows=nflows_Mdiff_NSF,\n",
    "        base_dist=base_dist_Mdiff,\n",
    "        mu_pos=True\n",
    "        )\n",
    "\n",
    "\n",
    "    ndim = ndim_diff + 1\n",
    "    model_comb_mass = COMBINED_Model(\n",
    "        None,\n",
    "        model_Mdiff,\n",
    "        # None,\n",
    "        model_M1,\n",
    "        model_BinaryMask,\n",
    "        model_multiclass,\n",
    "        ndim,\n",
    "        ksize,\n",
    "        ns_d,\n",
    "        ns_h,\n",
    "        1,\n",
    "        ninp,\n",
    "        nfeature_cnn,\n",
    "        nout_cnn,\n",
    "        layers_types=layers_types,\n",
    "        act='tanh',\n",
    "        padding='valid',\n",
    "        sep_Binary_cond=True,\n",
    "        sep_MultiClass_cond=True,\n",
    "        sep_M1_cond=True,\n",
    "        sep_Mdiff_cond=True,\n",
    "        num_cond_Binary = num_cond_Ntot,\n",
    "        num_cond_MultiClass = num_cond_Ntot,\n",
    "        num_cond_M1 = num_cond_M1,\n",
    "        num_cond_Mdiff = num_cond_Mdiff\n",
    "        ).to(device_id)\n",
    "\n",
    "    # model = DDP(model, device_ids=[device_id], find_unused_parameters=True)\n",
    "\n",
    "    model_comb_mass = torch.nn.DataParallel(model_comb_mass)\n",
    "    tf = time()\n",
    "    if verbose:\n",
    "        print(f\"Time to load the mass model: {tf - ti:.2f}\")\n",
    "    ti = time()\n",
    "\n",
    "    ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test0/'\n",
    "\n",
    "    checkpoint = torch.load(ldir_cp + f'test_model_bestfit_6484.pth', map_location=device_id)\n",
    "    # print(iter)\n",
    "    model_comb_mass.load_state_dict(checkpoint['state_dict'])\n",
    "    model_comb_mass.eval()\n",
    "\n",
    "    run_config_name = 'TRAIN_VEL_FREECOSMO_cond_fastpm_ns128.yaml'\n",
    "    with open(\"/mnt/home/spandey/ceph/CHARM/run_configs/\" + run_config_name,\"r\") as file_object:\n",
    "        config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "    config_net = config['network_settings']\n",
    "    K_vel = config_net['K_vel']\n",
    "    B_vel = config_net['B_vel']\n",
    "    nflows_vel_NSF = config_net['nflows_vel_NSF']\n",
    "    cond_Mass_for_vel = config_net['cond_Mass_for_vel']\n",
    "    base_dist_vel = config_net['base_dist_vel']\n",
    "    ndim_mass = Nmax\n",
    "    ndim_vel = 3*Nmax\n",
    "    if cond_Mass_for_vel:\n",
    "        num_cond_vel = num_cond + ndim_mass\n",
    "    else:\n",
    "        num_cond_vel = num_cond\n",
    "\n",
    "    model_vel = NSF_Autoreg_CNNcond(\n",
    "        dim=ndim_vel,\n",
    "        K=K_vel,\n",
    "        B=B_vel,\n",
    "        hidden_dim=hidden_dim_MAF,\n",
    "        num_cond=num_cond_vel,\n",
    "        nflows=nflows_vel_NSF,\n",
    "        base_dist=base_dist_vel,\n",
    "        mu_pos=False\n",
    "        )\n",
    "\n",
    "\n",
    "    # ndim = ndim_diff + 1\n",
    "    model_comb_vel = COMBINED_Model_vel_only(\n",
    "        None,\n",
    "        model_vel,\n",
    "        ndim_vel,\n",
    "        ksize,\n",
    "        ns_d,\n",
    "        ns_h,\n",
    "        1,\n",
    "        ninp,\n",
    "        nfeature_cnn,\n",
    "        nout_cnn,\n",
    "        layers_types=layers_types,\n",
    "        act='tanh',\n",
    "        padding='valid',\n",
    "        ).to(device_id)\n",
    "\n",
    "\n",
    "\n",
    "    model_comb_vel = torch.nn.DataParallel(model_comb_vel)\n",
    "\n",
    "    ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test2_vel/'\n",
    "    # ldir_cp = '/mnt/home/spandey/ceph/CHARM/model_checkpoints/test0_vel/'\n",
    "\n",
    "    checkpoint = torch.load(ldir_cp + f'test_model_bestfit_6600.pth', map_location=device_id)\n",
    "    # print(iter)\n",
    "    model_comb_vel.load_state_dict(checkpoint['state_dict'])\n",
    "    model_comb_vel.eval()\n",
    "\n",
    "\n",
    "    tf = time()\n",
    "    if verbose:\n",
    "        print(f\"Time to load the velocity model: {tf - ti:.2f}\")\n",
    "    ti = time()\n",
    "\n",
    "\n",
    "\n",
    "    ldir_f = '/mnt/home/spandey/ceph/Quijote/data_NGP_self_fastpm_LH/'\n",
    "\n",
    "    n_dim_red = (nf - 1) // 2\n",
    "    n_pad = n_dim_red * nc\n",
    "\n",
    "    df_zg = pk.load(open(ldir_f + '%d/density_HR_full_m_res_128_z=0.5_nbatch_8_nfilter_3_ncnn_0.pk'%test_id,'rb'))\n",
    "    df_test_zg = df_zg['density_cic_unpad_combined']\n",
    "    df_test_pad_zg = np.pad(df_test_zg, n_pad, 'wrap')\n",
    "\n",
    "\n",
    "    z_REDSHIFT = float(z_all_FP[-1].split('_')[1])\n",
    "    if z_REDSHIFT == 0.0:\n",
    "        z_REDSHIFT = 0\n",
    "\n",
    "    df_load = pk.load(open(\n",
    "        ldir_f + '/' + str(test_id) + '/velocity_HR_full_m_res_128_z=' + str(z_REDSHIFT) + '_nbatch_8_nfilter_3_ncnn_0.pk', 'rb')\n",
    "        )\n",
    "\n",
    "    vel_all = df_load['velocity_cic_unpad_combined']\n",
    "    # # vel_pad_all = \n",
    "    vel_pad = np.stack([np.pad(vel_all[j,...], n_pad, 'wrap') for j in range(3)], axis=0)\n",
    "\n",
    "    df_test_all_pad = np.concatenate([np.log(1 + df_test_pad_zg + 1e-10)[None,...], vel_pad], axis=0)[None, None,:]\n",
    "    # df_test_all_pad = np.stack([np.log(1 + df_test_pad_zg + 1e-10), np.log(\n",
    "    #     1 + df_test_pad_zIC + 1e-10), df_test_pad_constrast_zg], axis=0)[None, None, :]\n",
    "\n",
    "    # df_test_all_pad.shape\n",
    "\n",
    "    # density_smoothed = gaussian_filter(df_test_zg, sigma=VALUE_SIG)\n",
    "    # df_test_constrast_zg = density_smoothed - df_test_zg\n",
    "    # \n",
    "    df_test_all_unpad = np.concatenate([np.log(1 + df_test_zg + 1e-10)[None,...], vel_all], axis=0)[None, None,:]\n",
    "    # df_test_all_unpad = np.stack([np.log(1 + df_test_zg + 1e-10), np.log(\n",
    "    #     1 + df_test_zIC + 1e-10), df_test_constrast_zg], axis=0)[None, None, :]\n",
    "\n",
    "    # df_test_all_unpad.shape\n",
    "\n",
    "    cond_nsh_test = np.moveaxis(df_test_all_unpad, 2, 5)\n",
    "    nsims_test = cond_nsh_test.shape[1]\n",
    "    nax_h_test = cond_nsh_test.shape[2]\n",
    "    ninp_test = cond_nsh_test.shape[-1]\n",
    "    cond_tensor_nsh_test = torch.Tensor(np.copy(cond_nsh_test.reshape(1,nsims_test * (nax_h_test ** 3), ninp_test))).to(device_id)    \n",
    "\n",
    "    # cond_tensor_nsh_test.shape\n",
    "    LH_cosmo_val_file='/mnt/home/spandey/ceph/Quijote/latin_hypercube_params.txt'\n",
    "    LH_cosmo_val_all = np.loadtxt(LH_cosmo_val_file)\n",
    "    fid_cosmo_val_all = LH_cosmo_val_all[test_id]\n",
    "    # fid_cosmo_val_all = np.array([0.3175, 0.049, 0.6711, 0.9624, 0.64])  \n",
    "\n",
    "    cosmo_val_test = np.tile(fid_cosmo_val_all, (cond_tensor_nsh_test.shape[1] ,1))[None,:]\n",
    "\n",
    "    # cosmo_val_test.shape\n",
    "    # df_test_all_pad.shape, df_test_all_unpad.shape, cosmo_val_test.shape\n",
    "    df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
    "    df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(device_id)\n",
    "    cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n",
    "\n",
    "\n",
    "    train_Ntot, train_M1, train_Mdiff = 1, 1, 1\n",
    "    train_binary, train_multi = 1, 1\n",
    "    # if verbose:\n",
    "        # print(f\"Running the model\")\n",
    "\n",
    "    # run the model\n",
    "    Ntot_samp_test, M1_samp_test, M_diff_samp_test, mask_tensor_M1_samp_test, mask_tensor_Mdiff_samp_test, _ = model_comb_mass.module.inverse(\n",
    "        cond_x=df_test_all_pad,\n",
    "        cond_x_nsh=df_test_all_unpad,\n",
    "        cond_cosmo=cosmo_val_test,\n",
    "        use_truth_Nhalo=1-train_Ntot,\n",
    "        use_truth_M1=1-train_M1,\n",
    "        use_truth_Mdiff=1-train_Mdiff,\n",
    "        mask_Mdiff_truth=None,\n",
    "        mask_M1_truth=None,\n",
    "        Nhalos_truth=None,\n",
    "        M1_truth=None,\n",
    "        Mdiff_truth=None,\n",
    "        train_binary=train_binary,\n",
    "        train_multi=train_multi,\n",
    "        train_M1=train_M1,\n",
    "        train_Mdiff=train_Mdiff,\n",
    "    )\n",
    "\n",
    "    tf = time()\n",
    "    if verbose:\n",
    "        print(f\"Time to sample from the mass model: {tf - ti:.2f}\")\n",
    "    ti = time()\n",
    "\n",
    "    BoxSize=1000\n",
    "    # Ntot_samp_test[0].shape\n",
    "    Ntot_samp_test_rs = Ntot_samp_test[0][:, np.newaxis]\n",
    "    M1_samp_test_rs = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:, 0]).cpu().detach().numpy()\n",
    "    M_diff_samp_test_rs = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ntot_samp_test = Ntot_samp_test[0][:, np.newaxis]\n",
    "    save_subvol_Nhalo = Ntot_samp_test.reshape(\n",
    "        nsims_test, nax_h_test, nax_h_test, nax_h_test)\n",
    "    save_subvol_M1 = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:, 0]\n",
    "                        ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, 1)\n",
    "    save_subvol_Mdiff = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]\n",
    "                            ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff)\n",
    "\n",
    "    mask_subvol_Mtot1 = mask_tensor_M1_samp_test[0].cpu().detach().numpy().reshape(\n",
    "        nsims_test, nax_h_test, nax_h_test, nax_h_test)[..., None]\n",
    "    mask_subvol_Mtot2 = mask_tensor_Mdiff_samp_test[0].cpu().detach().numpy(\n",
    "    ).reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff)\n",
    "    mask_subvol_Mtot = np.concatenate(\n",
    "        [mask_subvol_Mtot1, mask_subvol_Mtot2], axis=-1)\n",
    "\n",
    "    # compute the mass of halos from output\n",
    "    save_subvol_Mtot = np.zeros(\n",
    "        (nsims_test, nax_h_test, nax_h_test, nax_h_test, ndim_diff + 1))\n",
    "    # Mmin, Mmax = return_dict_test['Mmin'], return_dict_test['Mmax']\n",
    "    for jd in range(ndim_diff + 1):\n",
    "        if jd == 0:\n",
    "            save_subvol_Mtot[..., jd] = (\n",
    "                save_subvol_M1[..., 0] + 0.5) * (lgMmax - lgMmin) + lgMmin\n",
    "        else:\n",
    "            save_subvol_Mtot[...,\n",
    "                                jd] = (save_subvol_Mtot[..., jd - 1]) - (save_subvol_Mdiff[..., jd - 1]) * (lgMmax - lgMmin)\n",
    "\n",
    "    save_subvol_Mtot *= mask_subvol_Mtot\n",
    "\n",
    "    Nhalos = save_subvol_Nhalo[0, ...]  # histogram of halos in each voxel\n",
    "    M_halos = save_subvol_Mtot[0, ...]  # mass of halos in each voxel\n",
    "    M_halos_sort_norm = rescale_sub + (M_halos - lgMmin)/(lgMmax - lgMmin)\n",
    "    M_halos_sort_norm *= mask_subvol_Mtot[0, ...]\n",
    "    M_halos_sort_norm_condvel = M_halos_sort_norm.reshape(nax_h_test**3, -1)\n",
    "    # create the meshgrid\n",
    "    xall = (np.linspace(0, BoxSize, ns_h + 1))\n",
    "    xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "    yarray = np.copy(xarray)\n",
    "    zarray = np.copy(xarray)\n",
    "    x_cy, y_cy, z_cy = np.meshgrid(xarray, yarray, zarray, indexing='ij')\n",
    "\n",
    "    # record discrete halo positions and masses\n",
    "    x_h_mock, y_h_mock, z_h_mock, lgM_mock = [], [], [], []\n",
    "    # Nmax_sel = 3\n",
    "    k = 0\n",
    "    for jx in range(ns_h):\n",
    "        for jy in range(ns_h):\n",
    "            for jz in range(ns_h):\n",
    "                Nh_vox = int(Nhalos[jx, jy, jz])\n",
    "                if Nh_vox > 0:\n",
    "                    x_h_mock.append(x_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                    y_h_mock.append(y_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                    z_h_mock.append(z_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "\n",
    "                    lgM_mock.append((M_halos[jx, jy, jz, :Nh_vox]))\n",
    "                    k += Nh_vox\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    x_h_mock = np.concatenate(x_h_mock)\n",
    "    y_h_mock = np.concatenate(y_h_mock)\n",
    "    z_h_mock = np.concatenate(z_h_mock)\n",
    "    pos_h_mock = np.vstack((x_h_mock, y_h_mock, z_h_mock)).T\n",
    "    lgMass_mock = np.concatenate(lgM_mock)\n",
    "    # convert to float data type\n",
    "    pos_h_mock = pos_h_mock.astype('float32')\n",
    "    lgMass_mock = lgMass_mock.astype('float32')\n",
    "    tf = time()\n",
    "    if verbose:\n",
    "        print(f\"Time to process masses/numbers: {tf - ti:.2f}\")\n",
    "    ti = time()\n",
    "    \n",
    "\n",
    "    df_test_all_pad = torch.tensor(df_test_all_pad).to(device_id)\n",
    "    df_test_all_unpad = torch.tensor(df_test_all_unpad).to(device_id)\n",
    "    cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(device_id)\n",
    "    Nhalos_truth_recomb_tensor = torch.Tensor(Ntot_samp_test_rs[None,...]).to(device_id)\n",
    "    if cond_Mass_for_vel:\n",
    "        Mhalos_truth_recomb_tensor = torch.Tensor(M_halos_sort_norm_condvel[None,...]).to(device_id)\n",
    "    else:\n",
    "        Mhalos_truth_recomb_tensor = None\n",
    "\n",
    "    vel_samp_out = model_comb_vel.module.inverse(cond_x=df_test_all_pad,\n",
    "                                cond_x_nsh=df_test_all_unpad,\n",
    "                                cond_cosmo=cosmo_val_test,\n",
    "                                mask_vel_truth=None,\n",
    "                                Nhalos_truth=Nhalos_truth_recomb_tensor,\n",
    "                                Mhalos_truth=Mhalos_truth_recomb_tensor,\n",
    "                                # Mhalos_truth=None,                            \n",
    "                                vel_truth=None)\n",
    "    tf = time()\n",
    "    if verbose:\n",
    "        print(f\"Time to sample from the velocity model: {tf - ti:.2f}\")\n",
    "    ti = time()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    import pickle as pk\n",
    "    from scipy.interpolate import RegularGridInterpolator\n",
    "    grid = 128\n",
    "    BoxSize = 1000.\n",
    "    xall = (np.linspace(0, BoxSize, grid + 1))\n",
    "    xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "    yarray = np.copy(xarray)\n",
    "    zarray = np.copy(xarray)\n",
    "\n",
    "    vel_load_dir = '/mnt/home/spandey/ceph/Quijote/data_NGP_self_fastpm_LH/'\n",
    "    df = pk.load(open(vel_load_dir + f'{test_id}/velocity_HR_full_m_res_128_z=0.5_nbatch_8_nfilter_3_ncnn_0.pk', 'rb'))['velocity_cic_unpad_combined']\n",
    "    # df.shape\n",
    "    vx_mesh_load = 1000.*df[0,...]\n",
    "    vy_mesh_load = 1000.*df[1,...]\n",
    "    vz_mesh_load = 1000.*df[2,...]\n",
    "\n",
    "    vx_all_3D_interp_l = RegularGridInterpolator((xarray, yarray, zarray), vx_mesh_load, bounds_error=False, fill_value=None)\n",
    "    vy_all_3D_interp_l = RegularGridInterpolator((xarray, yarray, zarray), vy_mesh_load, bounds_error=False, fill_value=None)\n",
    "    vz_all_3D_interp_l = RegularGridInterpolator((xarray, yarray, zarray), vz_mesh_load, bounds_error=False, fill_value=None)\n",
    "\n",
    "    vx_eval_interp_l = vx_all_3D_interp_l(pos_h_mock)\n",
    "    vy_eval_interp_l = vy_all_3D_interp_l(pos_h_mock)\n",
    "    vz_eval_interp_l = vz_all_3D_interp_l(pos_h_mock)\n",
    "\n",
    "    vmax = 1000\n",
    "    vmin = -1000\n",
    "    jsim = test_id\n",
    "    ndim_diff = Nmax - 1\n",
    "    sdir_fid='/mnt/home/spandey/ceph/Quijote/data_NGP_self_LH'\n",
    "    mass_type = 'rockstar_200c'\n",
    "    # try:\n",
    "    fname = sdir_fid + '/' + str(jsim) + '/SPARSEMATS_halos_HR_rockstar_200c_lgMmincut_5e12_full_res_128_z=0.5.pk'\n",
    "    df_h_full = pk.load(open(fname, 'rb'))\n",
    "\n",
    "    df_Mh_all_truth_comb, df_Nh_truth_comb = df_h_full['M_halos_combined'].todense(), df_h_full['N_halos_combined'].todense()\n",
    "    v_halos_pred_recomb = df_h_full['v_halos_pred_combined'].todense()\n",
    "    v_halos_truth_recomb = df_h_full['v_halos_true_combined'].todense()\n",
    "\n",
    "    Nhalos_pred_recomb = Ntot_samp_test_rs[...,0].reshape(ns_h, ns_h, ns_h)\n",
    "\n",
    "    Nhalos_truth_recomb = np.clip(df_Nh_truth_comb, 0, Nmax)\n",
    "    M_halos_truth_recomb = df_Mh_all_truth_comb[...,:Nmax]\n",
    "\n",
    "\n",
    "    # v_halos_diff_recomb = np.reshape(vel_samp_out, (1, 128, 128, 128, ndim_diff + 1))[0,...]\n",
    "    v_halos_diff_recomb = np.reshape(vel_samp_out, (1, 128, 128, 128, (ndim_diff + 1)*3))[0,...]\n",
    "    v_halos_diff_recomb = np.reshape(v_halos_diff_recomb, (128, 128, 128, (ndim_diff + 1), 3))\n",
    "\n",
    "\n",
    "    ind_cart_lin = {}\n",
    "    ind_lin_cart = {}\n",
    "    jc = 0\n",
    "    # from tqdm import tqdm\n",
    "    for jx in (range(nb)):\n",
    "        for jy in range(nb):\n",
    "            for jz in range(nb):\n",
    "                # get the sub-cube\n",
    "                ind_lin_cart[(jx, jy, jz)] = jc\n",
    "                ind_cart_lin[jc] = [jx, jy, jz]\n",
    "                jc += 1\n",
    "\n",
    "    xall = (np.linspace(0, 1000, ns_h + 1))\n",
    "    xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "    yarray = np.copy(xarray)\n",
    "    zarray = np.copy(xarray)\n",
    "    x_cy, y_cy, z_cy = np.meshgrid(xarray, yarray, zarray, indexing='ij')\n",
    "\n",
    "\n",
    "    x_h_truth, y_h_truth, z_h_truth, lgM_truth,  = [], [], [], []\n",
    "    vx_truth_orig, vx_pred_orig = [], []\n",
    "    vx_diff_orig = []\n",
    "    vx_wodiff_orig = []\n",
    "\n",
    "    vy_truth_orig, vy_pred_orig = [], []\n",
    "    vy_diff_orig = []\n",
    "    vy_wodiff_orig = []\n",
    "\n",
    "    vz_truth_orig, vz_pred_orig = [], []\n",
    "    vz_diff_orig = []\n",
    "    vz_wodiff_orig = []\n",
    "\n",
    "    for jx in range(ns_h):\n",
    "        for jy in range(ns_h):\n",
    "            for jz in range(ns_h):\n",
    "                    Nh_vox = int(Nhalos_truth_recomb[jx, jy, jz])\n",
    "                    # Nh_vox = int(df_Nh_truth_comb[jx, jy, jz])                    \n",
    "                    if Nh_vox > 0:\n",
    "                        # if Nh_vox > Nmax_sel:\n",
    "                        #     Nh_vox = Nmax_sel\n",
    "\n",
    "                        x_h_truth.append(x_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                        y_h_truth.append(y_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                        z_h_truth.append(z_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                        \n",
    "                        lgM_truth.append((M_halos_truth_recomb[jx, jy, jz, :Nh_vox]))\n",
    "\n",
    "                        vx_truth_orig.append((v_halos_truth_recomb[...,0][jx, jy, jz, :Nh_vox]))\n",
    "                        vx_diff_orig.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 0])*((vmax - vmin)) + vmin))\n",
    "                        vx_pred_here = (v_halos_pred_recomb[...,0][jx, jy, jz, :Nh_vox]) - ((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 0])*((vmax - vmin)) + vmin)\n",
    "                        vx_pred_orig.append((vx_pred_here))\n",
    "                        vx_wodiff_orig.append(v_halos_pred_recomb[...,0][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "\n",
    "                        vy_truth_orig.append((v_halos_truth_recomb[...,1][jx, jy, jz, :Nh_vox]))\n",
    "                        vy_diff_orig.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 1])*((vmax - vmin)) + vmin))\n",
    "                        vy_pred_here = (v_halos_pred_recomb[...,1][jx, jy, jz, :Nh_vox]) - ((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 1])*((vmax - vmin)) + vmin)\n",
    "                        vy_pred_orig.append((vy_pred_here))\n",
    "                        vy_wodiff_orig.append(v_halos_pred_recomb[...,1][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "                        vz_truth_orig.append((v_halos_truth_recomb[...,2][jx, jy, jz, :Nh_vox]))\n",
    "                        vz_diff_orig.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 2])*((vmax - vmin)) + vmin))\n",
    "                        vz_pred_here = (v_halos_pred_recomb[...,2][jx, jy, jz, :Nh_vox]) - ((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 2])*((vmax - vmin)) + vmin)\n",
    "                        vz_pred_orig.append((vz_pred_here))\n",
    "                        vz_wodiff_orig.append(v_halos_pred_recomb[...,2][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # x_h_truth, y_h_truth, z_h_truth, lgM_truth, vx_truth_orig, vx_pred_orig = [], [], [], [], [], []\n",
    "    vx_diff_mock = []\n",
    "    # vx_wodiff_orig = []\n",
    "\n",
    "    vy_diff_mock = []\n",
    "    # vy_wodiff_orig = []\n",
    "\n",
    "    vz_diff_mock = []\n",
    "    # vz_wodiff_orig = []\n",
    "\n",
    "\n",
    "    for jx in range(ns_h):\n",
    "        for jy in range(ns_h):\n",
    "            for jz in range(ns_h):\n",
    "                    Nh_vox = int(Nhalos_pred_recomb[jx, jy, jz])\n",
    "                    # Nh_vox = int(df_Nh_truth_comb[jx, jy, jz])                    \n",
    "                    if Nh_vox > 0:\n",
    "                        vx_diff_mock.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 0])*((vmax - vmin)) + vmin))\n",
    "\n",
    "                        vy_diff_mock.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 1])*((vmax - vmin)) + vmin))\n",
    "\n",
    "                        vz_diff_mock.append(((v_halos_diff_recomb[jx, jy, jz, :Nh_vox, 2])*((vmax - vmin)) + vmin))\n",
    "\n",
    "                        # vx_wodiff_orig.append(v_halos_pred_recomb[...,0][jx, jy, jz, :Nh_vox])                                        \n",
    "\n",
    "\n",
    "    # vx_eval_interp_l = vx_all_3D_interp_l(pos_h_mock)\n",
    "    # pos_h_mock.shape\n",
    "    vx_total_mock = vx_eval_interp_l - np.concatenate(vx_diff_mock)\n",
    "\n",
    "    vy_total_mock = vy_eval_interp_l - np.concatenate(vy_diff_mock)\n",
    "\n",
    "    vz_total_mock = vz_eval_interp_l - np.concatenate(vz_diff_mock)\n",
    "\n",
    "    tf = time()\n",
    "    if verbose:\n",
    "        print(f\"Time to process velocities: {tf - ti:.2f}\")\n",
    "    ti = time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690403,), (690403,), (690403,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vx_total_mock.shape, vy_total_mock.shape, vz_total_mock.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_h_mock = np.vstack((vx_total_mock, vy_total_mock, vz_total_mock)).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the truth halos: 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49182/3707334875.py:16: RuntimeWarning: divide by zero encountered in log10\n",
      "  lgMass_truth = np.log10(mass_truth).astype(np.float32)\n"
     ]
    }
   ],
   "source": [
    "isim = test_id\n",
    "# snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/fiducial_HR'\n",
    "snap_dir_base = '/mnt/home/fvillaescusa/ceph/Quijote/Halos/Rockstar/latin_hypercube_HR'\n",
    "snapnum = 3\n",
    "# MAS     = 'NGP'  #mass-assigment scheme\n",
    "# verbose = False   #print information on progress\n",
    "snapdir = snap_dir_base + '/' + str(isim)  #folder hosting the catalogue\n",
    "rockstar = np.loadtxt(snapdir + '/out_' + str(snapnum) + '_pid.list')\n",
    "with open(snapdir + '/out_' + str(snapnum) + '_pid.list', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "header = lines[0].split()\n",
    "# get the properties of the halos\n",
    "pos_h_truth = rockstar[:,header.index('X'):header.index('Z')+1]\n",
    "index_M = header.index('M200c')                    \n",
    "mass_truth = rockstar[:,index_M]  #Halo masses in Msun/h\n",
    "lgMass_truth = np.log10(mass_truth).astype(np.float32)\n",
    "vel_h_truth = rockstar[:,header.index('VX'):header.index('VZ')+1]\n",
    "\n",
    "indsel = np.where(mass_truth > 5e12)[0]\n",
    "pos_h_truth = pos_h_truth[indsel]\n",
    "vel_h_truth = vel_h_truth[indsel]\n",
    "lgMass_truth = lgMass_truth[indsel]\n",
    "tf = time()\n",
    "if verbose:\n",
    "    print(f\"Time to load the truth halos: {tf - ti:.2f}\")\n",
    "ti = time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_space_library as RSL\n",
    "BoxSize = 1000.\n",
    "Hubble = fid_cosmo_val_all[2]*100.\n",
    "redshift = 0.5\n",
    "\n",
    "axis = 0\n",
    "velx_orig = (vel_h_truth[:,0]).astype(np.float32)[:, None]\n",
    "velx_charm = (vx_total_mock).astype(np.float32)[:, None]\n",
    "pos_rsdx_orig = np.copy(pos_h_truth).astype(np.float32)\n",
    "pos_rsdx_charm = np.copy(np.ascontiguousarray(pos_h_mock)).astype(np.float32)\n",
    "RSL.pos_redshift_space(pos_rsdx_orig, velx_orig, BoxSize, Hubble, redshift, axis)\n",
    "RSL.pos_redshift_space(pos_rsdx_charm, velx_charm, BoxSize, Hubble, redshift, axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_space_library as RSL\n",
    "axis = 1\n",
    "velx_orig = (vel_h_truth[:,1]).astype(np.float32)[:, None]\n",
    "velx_charm = (vy_total_mock).astype(np.float32)[:, None]\n",
    "pos_rsdy_orig = np.copy(pos_h_truth).astype(np.float32)\n",
    "pos_rsdy_charm = np.copy(np.ascontiguousarray(pos_h_mock)).astype(np.float32)\n",
    "RSL.pos_redshift_space(pos_rsdy_orig, velx_orig, BoxSize, Hubble, redshift, axis)\n",
    "RSL.pos_redshift_space(pos_rsdy_charm, velx_charm, BoxSize, Hubble, redshift, axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 2\n",
    "velx_orig = (vel_h_truth[:,2]).astype(np.float32)[:, None]\n",
    "velx_charm = (vz_total_mock).astype(np.float32)[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rsdz_orig = np.copy(pos_h_truth).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_space_library as RSL\n",
    "RSL.pos_redshift_space(pos_rsdz_orig, velx_orig, BoxSize, Hubble, redshift, axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rsdz_charm = np.copy(np.ascontiguousarray(pos_h_mock)).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_space_library as RSL\n",
    "RSL.pos_redshift_space(pos_rsdz_charm, velx_charm, BoxSize, Hubble, redshift, axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ili-sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
